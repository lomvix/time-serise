{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference:  [挑战Transformer！Mamba的架构及实现(Pytorch）](https://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247630782&idx=1&sn=82a2d74642278e0fb03f0954a03b7692&chksm=e8d0e224b82c7033404262318f4c8a6f3f50db716d32dc6e4f3fb870bca2fe5b757f9d1940d7&scene=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x267922e64b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "import os\n",
    "import urllib.request\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration flags and hyperparameters\n",
    "USE_MAMBA = 1\n",
    "DIFFERENT_H_STATES_RECURRENT_UPDATE_MECHANISM = 0\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 8\n",
    "state_size = 128 # Example state size\n",
    "seq_len = 100 # Example sequence length\n",
    "batch_size = 256 # Example batch size\n",
    "last_batch_size = 81 # only for the very last batch of the dataset\n",
    "current_batch_size = batch_size\n",
    "different_batch_size = False\n",
    "h_new = None\n",
    "temp_buffer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S6(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, state_size, device):\n",
    "        super(S6, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(d_model, d_model, device=device)\n",
    "        self.fc2 = nn.Linear(d_model, state_size, device=device)\n",
    "        self.fc3 = nn.Linear(d_model, state_size, device=device)\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        self.state_size = state_size\n",
    "\n",
    "\n",
    "        self.A = nn.Parameter(F.normalize(torch.ones(d_model, state_size, device=device), p=2, dim=-1))\n",
    "        nn.init.xavier_uniform_(self.A)\n",
    "\n",
    "        self.B = torch.zeros(batch_size, self.seq_len, self.state_size, device=device)\n",
    "        self.C = torch.zeros(batch_size, self.seq_len, self.state_size, device=device)\n",
    "\n",
    "        self.delta = torch.zeros(batch_size, self.seq_len, self.d_model, device=device)\n",
    "        self.dA = torch.zeros(batch_size, self.seq_len, self.d_model, self.state_size, device=device)\n",
    "        self.dB = torch.zeros(batch_size, self.seq_len, self.d_model, self.state_size, device=device)\n",
    "\n",
    "        # h [batch_size, seq_len, d_model, state_size]\n",
    "        self.h = torch.zeros(batch_size, self.seq_len, self.d_model, self.state_size, device=device)\n",
    "        self.y = torch.zeros(batch_size, self.seq_len, self.d_model, device=device)\n",
    "\n",
    "\n",
    "    def discretization(self):\n",
    "\n",
    "        self.dB = torch.einsum(\"bld,bln->bldn\", self.delta, self.B)\n",
    "\n",
    "        self.dA = torch.exp(torch.einsum(\"bld,dn->bldn\", self.delta, self.A))\n",
    "\n",
    "\n",
    "        return self.dA, self.dB\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Algorithm 2 MAMBA paper\n",
    "        self.B = self.fc2(x)\n",
    "        self.C = self.fc3(x)\n",
    "        self.delta = F.softplus(self.fc1(x))\n",
    "\n",
    "        self.discretization()\n",
    "\n",
    "        if DIFFERENT_H_STATES_RECURRENT_UPDATE_MECHANISM:  \n",
    "\n",
    "            global current_batch_size\n",
    "            current_batch_size = x.shape[0]\n",
    "\n",
    "            if self.h.shape[0] != current_batch_size:\n",
    "                different_batch_size = True\n",
    "\n",
    "                h_new =  torch.einsum('bldn,bldn->bldn', self.dA, self.h[:current_batch_size, ...]) + rearrange(x, \"b l d -> b l d 1\") * self.dB\n",
    "\n",
    "            else:\n",
    "                different_batch_size = False\n",
    "                h_new =  torch.einsum('bldn,bldn->bldn', self.dA, self.h) + rearrange(x, \"b l d -> b l d 1\") * self.dB\n",
    "\n",
    "            # y [batch_size, seq_len, d_model]\n",
    "            self.y = torch.einsum('bln,bldn->bld', self.C, h_new)\n",
    "\n",
    "            global temp_buffer\n",
    "            temp_buffer = h_new.detach().clone() if not self.h.requires_grad else h_new.clone()\n",
    "\n",
    "            return self.y\n",
    "\n",
    "        else:  \n",
    "            # h [batch_size, seq_len, d_model, state_size]\n",
    "            h = torch.zeros(x.size(0), self.seq_len, self.d_model, self.state_size, device=x.device)\n",
    "            y = torch.zeros_like(x)\n",
    "\n",
    "            h =  torch.einsum('bldn,bldn->bldn', self.dA, h) + rearrange(x, \"b l d -> b l d 1\") * self.dB\n",
    "\n",
    "            # y [batch_size, seq_len, d_model]\n",
    "            y = torch.einsum('bln,bldn->bld', self.C, h)\n",
    "\n",
    "            return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,\n",
    "                d_model: int,\n",
    "                eps: float = 1e-5,\n",
    "                device: str ='cuda'):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        device = \"cpu\"\n",
    "        self.weight = nn.Parameter(torch.ones(d_model, device=device))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, state_size, device):\n",
    "        super(MambaBlock, self).__init__()\n",
    "\n",
    "        self.inp_proj = nn.Linear(d_model, 2*d_model, device=device)\n",
    "        self.out_proj = nn.Linear(2*d_model, d_model, device=device)\n",
    "\n",
    "        # For residual skip connection\n",
    "        self.D = nn.Linear(d_model, 2*d_model, device=device)\n",
    "\n",
    "        # Set _no_weight_decay attribute on bias\n",
    "        self.out_proj.bias._no_weight_decay = True\n",
    "\n",
    "        # Initialize bias to a small constant value\n",
    "        nn.init.constant_(self.out_proj.bias, 1.0)\n",
    "\n",
    "        self.S6 = S6(seq_len, 2*d_model, state_size, device)\n",
    "\n",
    "        # Add 1D convolution with kernel size 3\n",
    "        self.conv = nn.Conv1d(seq_len, seq_len, kernel_size=3, padding=1, device=device)\n",
    "\n",
    "        # Add linear layer for conv output\n",
    "        self.conv_linear = nn.Linear(2*d_model, 2*d_model, device=device)\n",
    "\n",
    "        # rmsnorm\n",
    "        self.norm = RMSNorm(d_model, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "    x_proj.shape = torch.Size([batch_size, seq_len, 2*d_model])\n",
    "    x_conv.shape = torch.Size([batch_size, seq_len, 2*d_model])\n",
    "    x_conv_act.shape = torch.Size([batch_size, seq_len, 2*d_model])\n",
    "    \"\"\"\n",
    "        # Refer to Figure 3 in the MAMBA paper\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x_proj = self.inp_proj(x)\n",
    "\n",
    "        # Add 1D convolution with kernel size 3\n",
    "        x_conv = self.conv(x_proj)\n",
    "\n",
    "        x_conv_act = F.silu(x_conv)\n",
    "\n",
    "        # Add linear layer for conv output\n",
    "        x_conv_out = self.conv_linear(x_conv_act)\n",
    "\n",
    "        x_ssm = self.S6(x_conv_out)\n",
    "        x_act = F.silu(x_ssm)  # Swish activation can be implemented as x * sigmoid(x)\n",
    "\n",
    "        # residual skip connection with nonlinearity introduced by multiplication\n",
    "        x_residual = F.silu(self.D(x))\n",
    "\n",
    "        x_combined = x_act * x_residual\n",
    "\n",
    "        x_out = self.out_proj(x_combined)\n",
    "\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mamba(nn.Module):\n",
    "     def __init__(self, seq_len, d_model, state_size, device):\n",
    "         super(Mamba, self).__init__()\n",
    "         self.mamba_block1 = MambaBlock(seq_len, d_model, state_size, device)\n",
    "         self.mamba_block2 = MambaBlock(seq_len, d_model, state_size, device)\n",
    "         self.mamba_block3 = MambaBlock(seq_len, d_model, state_size, device)\n",
    "\n",
    "     def forward(self, x):\n",
    "         x = self.mamba_block1(x)\n",
    "         x = self.mamba_block2(x)\n",
    "         x = self.mamba_block3(x)\n",
    "         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_output.shape = torch.Size([256, 100, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(batch_size, seq_len, d_model, device=device)\n",
    "# Create the Mamba model\n",
    "mamba = Mamba(seq_len, d_model, state_size,device=device)\n",
    "\n",
    "# rmsnorm\n",
    "norm = RMSNorm(d_model)\n",
    "x = norm(x)\n",
    "\n",
    "# Forward pass\n",
    "test_output = mamba(x)\n",
    "print(f\"test_output.shape = {test_output.shape}\")  # Should be [batch_size, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enwiki8Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.data.items()}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for padding\n",
    "def pad_sequences_3d(sequences, max_len=None, pad_value=0):\n",
    "    # Assuming sequences is a tensor of shape (batch_size, seq_len, feature_size)\n",
    "    batch_size, seq_len, feature_size = sequences.shape\n",
    "\n",
    "    if max_len is None:\n",
    "        max_len = seq_len + 1\n",
    "\n",
    "\n",
    "    # Initialize padded_sequences with the pad_value\n",
    "    padded_sequences = torch.full((batch_size, max_len, feature_size), fill_value=pad_value, dtype=sequences.dtype, device=sequences.device)\n",
    "    # Pad each sequence to the max_len\n",
    "    padded_sequences[:, :seq_len, :] = sequences\n",
    "\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, data_loader, optimizer, criterion, device, max_grad_norm=1.0, DEBUGGING_IS_ON=False):\n",
    "     model.train()\n",
    "     total_loss = 0\n",
    "     for batch in data_loader:\n",
    "         optimizer.zero_grad()\n",
    "\n",
    "         input_data = batch['input_ids'].clone().to(device)\n",
    "         attention_mask = batch['attention_mask'].clone().to(device)\n",
    "\n",
    "         target = input_data[:, 1:]\n",
    "         input_data = input_data[:, :-1]\n",
    "\n",
    "         # Pad all the sequences in the batch:\n",
    "         input_data = pad_sequences_3d(input_data, pad_value=tokenizer.pad_token_id)\n",
    "         target = pad_sequences_3d(target, max_len=input_data.size(1), pad_value=tokenizer.pad_token_id)\n",
    "\n",
    "         if USE_MAMBA:\n",
    "             output = model(input_data)\n",
    "             loss = criterion(output, target)\n",
    "\n",
    "         loss.backward(retain_graph=True)\n",
    "\n",
    "         for name, param in model.named_parameters():\n",
    "            if 'out_proj.bias' not in name:\n",
    "                # clip weights but not bias for out_proj\n",
    "                torch.nn.utils.clip_grad_norm_(param, max_norm=max_grad_norm)\n",
    "\n",
    "         if DEBUGGING_IS_ON:\n",
    "             for name, parameter in model.named_parameters():\n",
    "                 if parameter.grad is not None:\n",
    "                     print(f\"{name} gradient: {parameter.grad.data.norm(2)}\")\n",
    "                 else:\n",
    "                     print(f\"{name} has no gradient\")\n",
    "\n",
    "         if USE_MAMBA and DIFFERENT_H_STATES_RECURRENT_UPDATE_MECHANISM:\n",
    "             model.S6.h[:current_batch_size, ...].copy_(temp_buffer)\n",
    "\n",
    "         optimizer.step()\n",
    "\n",
    "         total_loss += loss.item()\n",
    "     return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion, device):\n",
    "     model.eval()\n",
    "     tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "     total_loss = 0\n",
    "     with torch.no_grad():\n",
    "         for batch in data_loader:\n",
    "             input_data = batch['input_ids'].clone().detach().to(device)\n",
    "             attention_mask = batch['attention_mask'].clone().detach().to(device)\n",
    "\n",
    "             target = input_data[:, 1:]\n",
    "             input_data = input_data[:, :-1]\n",
    "\n",
    "             # Pad all the sequences in the batch:\n",
    "             input_data = pad_sequences_3d(input_data, pad_value=tokenizer.pad_token_id)\n",
    "             target = pad_sequences_3d(target, max_len=input_data.size(1), pad_value=tokenizer.pad_token_id)\n",
    "\n",
    "             if USE_MAMBA:\n",
    "                 output = model(input_data)\n",
    "                 loss = criterion(output, target)\n",
    "             total_loss += loss.item()\n",
    "     return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(loss):\n",
    "     return math.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_enwiki8_dataset():\n",
    "     print(f\"Download and extract enwiki8 data\")\n",
    "     url = \"http://mattmahoney.net/dc/enwik8.zip\"\n",
    "     urllib.request.urlretrieve(url, \"enwik8.zip\")\n",
    "\n",
    "     with ZipFile(\"enwik8.zip\") as f:\n",
    "         data = f.read(\"enwik8\").decode(\"utf-8\")\n",
    "\n",
    "     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode the dataset\n",
    "def encode_dataset(tokenizer, text_data):\n",
    "    def batch_encode(tokenizer, text_data, batch_size=1000):\n",
    "        # Tokenize in batches\n",
    "        batched_input_ids = []\n",
    "        for i in range(0, len(text_data), batch_size):\n",
    "            batch = text_data[i:i+batch_size]\n",
    "            inputs = tokenizer(batch, add_special_tokens=True, truncation=True,\n",
    "                            padding='max_length', max_length=seq_len,\n",
    "                            return_tensors='pt')\n",
    "            batched_input_ids.append(inputs['input_ids'])\n",
    "        return torch.cat(batched_input_ids)\n",
    "\n",
    "    # Assuming enwiki8_data is a list of sentences\n",
    "    enwiki8_data=load_enwiki8_dataset()\n",
    "    input_ids = batch_encode(tokenizer, enwiki8_data)\n",
    "\n",
    "    # vocab_size is the number of unique tokens in the tokenizer's vocabulary\n",
    "    global vocab_size\n",
    "    vocab_size = len(tokenizer.vocab)  # Note that for some tokenizers, we might access the vocab directly\n",
    "    print(f\"vocab_size = {vocab_size}\")\n",
    "\n",
    "    # Create an embedding layer\n",
    "    # embedding_dim is the size of the embedding vectors (MAMBA model's D)\n",
    "    embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "\n",
    "    # Pass `input_ids` through the embedding layer\n",
    "    # This will change `input_ids` from shape [B, L] to [B, L, D]\n",
    "    def batch_embedding_calls(input_ids, embedding_layer, batch_size=256):\n",
    "        # Check if input_ids is already a tensor, if not convert it\n",
    "        if not isinstance(input_ids, torch.Tensor):\n",
    "            input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "\n",
    "        # Calculate the number of batches needed\n",
    "        num_batches = math.ceil(input_ids.size(0) / batch_size)\n",
    "\n",
    "        # List to hold the output embeddings\n",
    "        output_embeddings = []\n",
    "\n",
    "        # Process each batch\n",
    "        for i in range(num_batches):\n",
    "            # Calculate start and end indices for the current batch\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "\n",
    "            # Get the batch\n",
    "            input_id_batch = input_ids[start_idx:end_idx]\n",
    "\n",
    "            # Call the embedding layer\n",
    "            with torch.no_grad():  # No need gradients for this operation\n",
    "                batch_embeddings = embedding_layer(input_id_batch)\n",
    "\n",
    "            # Append the result to the list\n",
    "            output_embeddings.append(batch_embeddings)\n",
    "\n",
    "        # Concatenate the embeddings from each batch into a single tensor\n",
    "        all_embeddings = torch.cat(output_embeddings, dim=0)\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "    # `input_ids` is a list or tensor of the input IDs and `embedding_layer` is model's embedding layer\n",
    "    if USE_MAMBA:\n",
    "        # Set `batch_size` to a value that works for memory constraints\n",
    "        encoded_inputs = batch_embedding_calls(input_ids, embedding_layer, batch_size=1).float()\n",
    "\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).type(input_ids.dtype)\n",
    "\n",
    "    return encoded_inputs, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Assuming encoded_inputs is a preprocessed tensor of shape [num_samples, seq_len, d_model]\n",
    "encoded_inputs_file = 'encoded_inputs_mamba.pt'\n",
    "\n",
    "\n",
    "if os.path.exists(encoded_inputs_file):\n",
    "    print(\"Loading pre-tokenized data...\")\n",
    "    encoded_inputs = torch.load(encoded_inputs_file)\n",
    "else:\n",
    "    print(\"Tokenizing raw data...\")\n",
    "    enwiki8_data = load_enwiki8_dataset()\n",
    "    encoded_inputs, attention_mask = encode_dataset(tokenizer, enwiki8_data)\n",
    "    torch.save(encoded_inputs, encoded_inputs_file)\n",
    "    print(f\"finished tokenizing data\")\n",
    "\n",
    "\n",
    "# Combine into a single dictionary\n",
    "data = {\n",
    "    'input_ids': encoded_inputs,\n",
    "    'attention_mask': attention_mask\n",
    "}\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "total_size = len(data['input_ids'])\n",
    "train_size = int(total_size * 0.8)\n",
    "\n",
    "train_data = {key: val[:train_size] for key, val in data.items()}\n",
    "val_data = {key: val[train_size:] for key, val in data.items()}\n",
    "\n",
    "train_dataset = Enwiki8Dataset(train_data)\n",
    "val_dataset = Enwiki8Dataset(val_data)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "\n",
    "model = Mamba(seq_len, d_model, state_size, device).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 25  # Number of epochs to train for\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):  # loop over the dataset multiple times\n",
    "    train_loss = train(model, tokenizer, train_loader, optimizer, criterion, device, max_grad_norm=10.0, DEBUGGING_IS_ON=False)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    val_perplexity = calculate_perplexity(val_loss)\n",
    "    print(f'Epoch: {epoch+1}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Perplexity: {val_perplexity:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
