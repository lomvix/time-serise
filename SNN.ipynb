{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [SNN basic task](https://github.com/xjtulyc/SNN_basic_task)\n",
    "\n",
    "> [snntorch](https://pypi.org/project/snntorch/)\n",
    "\n",
    "> [SpikingJelly](https://spikingjelly.readthedocs.io/zh-cn/latest/index.html)\n",
    "\n",
    "目前测试到不适配回归任务\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "inp_size = 3\n",
    "out_size = 5\n",
    "state = torch.zeros(1, out_size) # statew = torch.zeros(inp_size, out_size) # weightsthreshold = 0.6\n",
    "x = torch.randn(1, inp_size) # input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNNLayer(nn.Module):\n",
    "        def __init__(self, inp_size, out_size, threshold=0.6):\n",
    "            super().__init__()                \n",
    "            self.threshold = threshold        \n",
    "            self.w = nn.Parameter(torch.randn(inp_size, out_size))        \n",
    "            self.w.requires_grad = False # don't apply gradient\n",
    "            self.state = torch.randn(1, out_size)        \n",
    "        def forward(self, x):        \n",
    "            out = x @ self.w # activ        \n",
    "            self.state = self.state*0.5 + out # state       \n",
    "            spikes = torch.where(self.state < self.threshold, 0, 1) # spike                \n",
    "            reset = -self.state * spikes        \n",
    "            self.state += reset # reset neuron which have a spike\n",
    "            # learning        \n",
    "            for n in range(spikes.shape[1]):# for each spike          \n",
    "                 if spikes[0, n] == 1:# if is activated                    \n",
    "                      for i in range(x.shape[1]): # for each input                        \n",
    "                           if x[0, i] > self.threshold: # if is > threshold                      \n",
    "                                self.w[i, n] += 0.05     # add a little value                  \n",
    "                           else:                      \n",
    "                                self.w[i, n] -= 0.05       # subtract a little value\n",
    "            return spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.randn(1, 5)+torch.randn(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7668],\n",
       "         [ 0.0668],\n",
       "         [-1.4182]],\n",
       "\n",
       "        [[-0.2947],\n",
       "         [-1.1553],\n",
       "         [-1.4004]],\n",
       "\n",
       "        [[-0.1301],\n",
       "         [-0.7960],\n",
       "         [ 0.6404]],\n",
       "\n",
       "        [[-0.9690],\n",
       "         [-0.2025],\n",
       "         [-0.7442]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(5,1).forward(torch.randn(4,3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m SNNLayer(\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mforward(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[1;32mIn[18], line 18\u001b[0m, in \u001b[0;36mSNNLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spikes[\u001b[38;5;241m0\u001b[39m, n] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\u001b[38;5;66;03m# if is activated                    \u001b[39;00m\n\u001b[0;32m     17\u001b[0m      \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]): \u001b[38;5;66;03m# for each input                        \u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m           \u001b[38;5;28;01mif\u001b[39;00m x[\u001b[38;5;241m0\u001b[39m, i] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold: \u001b[38;5;66;03m# if is > threshold                      \u001b[39;00m\n\u001b[0;32m     19\u001b[0m                \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw[i, n] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m     \u001b[38;5;66;03m# add a little value                  \u001b[39;00m\n\u001b[0;32m     20\u001b[0m           \u001b[38;5;28;01melse\u001b[39;00m:                      \n",
      "\u001b[1;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "SNNLayer(5,1).forward(torch.randn(4,3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8422,  1.9529, -0.8715,  2.7767, -2.2369],\n",
       "        [-0.8792,  1.0636,  2.8892,  0.4045, -2.5785],\n",
       "        [-0.9347,  0.1492,  0.5620,  3.2592, -1.7769]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 1, 0],\n",
       "        [0, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spikes = torch.where(state < 0.6, 0, 1)\n",
    "spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8422,  0.0000, -0.8715,  0.0000, -2.2369],\n",
       "        [-0.8792,  0.0000,  0.0000,  0.4045, -2.5785],\n",
       "        [-0.9347,  0.1492,  0.5620,  0.0000, -1.7769]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset = -state * spikes        \n",
    "state += reset\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spikes.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SNNLayer(inp_size=5,out_size=5).forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2512]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(in_features=5,out_features=1).forward(SNNLayer(inp_size=5,out_size=5).forward(x).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/3号机组发电态健康样本.csv\",encoding='GBK')\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "sc2 = MinMaxScaler()\n",
    "y=sc.fit_transform(np.array(data.iloc[:,1]).reshape(-1, 1))\n",
    "X=sc2.fit_transform(data.iloc[:,13:])\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(np.array(x_train),dtype=torch.float32), torch.tensor(np.array(y_train),dtype=torch.float32))\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(torch.tensor(np.array(x_test),dtype=torch.float32), torch.tensor(np.array(y_test),dtype=torch.float32))\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "train_data = train_loader\n",
    "test_data = test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8996, 9)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:,13:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0   Traintime : 6.3176329135894775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\26921\\AppData\\Local\\Temp\\ipykernel_26592\\1508783742.py:65: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\b\\abs_8f7uhuge1i\\croot\\pytorch-select_1717607507421\\work\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  average_loss =  criteon(torch.tensor(logit), torch.tensor(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1   Traintime : 4.583333730697632\n",
      "epoch 2   Traintime : 4.649648427963257\n",
      "epoch 3   Traintime : 4.89280366897583\n",
      "epoch 4   Traintime : 3.9275169372558594\n",
      "epoch 5   Traintime : 4.148637771606445\n",
      "epoch 6   Traintime : 4.245324373245239\n",
      "epoch 7   Traintime : 4.2588465213775635\n",
      "epoch 8   Traintime : 4.083964109420776\n",
      "epoch 9   Traintime : 4.196537256240845\n",
      "epoch 10   Traintime : 4.546640872955322\n",
      "epoch 11   Traintime : 4.834886312484741\n",
      "epoch 12   Traintime : 4.244397163391113\n",
      "epoch 13   Traintime : 4.8000593185424805\n",
      "epoch 14   Traintime : 4.43316125869751\n",
      "epoch 15   Traintime : 4.65178108215332\n",
      "epoch 16   Traintime : 4.988627672195435\n",
      "epoch 17   Traintime : 4.990806341171265\n",
      "epoch 18   Traintime : 4.27893328666687\n",
      "epoch 19   Traintime : 4.373445749282837\n"
     ]
    }
   ],
   "source": [
    "'''训练部分'''\n",
    "import torch.optim as optim\n",
    " \n",
    "feature_number = 9  # 设置特征数目\n",
    "out_prediction = 1  # 设置输出数目\n",
    "learning_rate = 0.01  # 设置学习率\n",
    "epochs = 20  # 设置训练代数\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output, n_neuron1, n_neuron2,n_layer):  # n_feature为特征数目，这个数字不能随便取,n_output为特征对应的输出数目，也不能随便取\n",
    "        self.n_feature=n_feature\n",
    "        self.n_output=n_output\n",
    "        self.n_neuron1=n_neuron1\n",
    "        self.n_neuron2=n_neuron2\n",
    "        self.n_layer=n_layer\n",
    "        super(Net, self).__init__()\n",
    "        self.input_layer = nn.Linear(self.n_feature, self.n_neuron1) # 输入层\n",
    "        self.hidden1 = nn.Linear(self.n_neuron1, self.n_neuron2) # 1类隐藏层    \n",
    "        self.hidden2 = SNNLayer(self.n_neuron2, self.n_neuron2) # 2类隐藏\n",
    "        self.predict = nn.Linear(self.n_neuron2, self.n_output) # 输出层\n",
    " \n",
    "    def forward(self, x):\n",
    "        '''定义前向传递过程'''\n",
    "        out = self.input_layer(x)\n",
    "        out = torch.relu(out) # 使用relu函数非线性激活\n",
    "        out = self.hidden1(out)\n",
    "        out = torch.relu(out)\n",
    "        for i in range(self.n_layer):\n",
    "            out = self.hidden2(out)\n",
    "            out = torch.relu(out)\n",
    "        out = out.float()\n",
    "        out = self.predict( # 回归问题最后一层不需要激活函数\n",
    "            out\n",
    "        )  # 除去feature_number与out_prediction不能随便取，隐藏层数与其他神经元数目均可以适当调整以得到最佳预测效果\n",
    "        return out\n",
    " \n",
    "net = Net(n_feature=feature_number,\n",
    "                      n_output=out_prediction,\n",
    "                      n_layer=1,\n",
    "                      n_neuron1=16,\n",
    "                      n_neuron2=8) # 这里直接确定了隐藏层数目以及神经元数目，实际操作中需要遍历\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)  # 使用Adam算法更新参数\n",
    "criteon = torch.nn.MSELoss()  # 误差计算公式，回归问题采用均方误差\n",
    "\n",
    "for epoch in range(epochs):  # 整个数据集迭代次数\n",
    "    net.train() # 启动训练模式\n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_data):\n",
    "        logits = net.forward(data)  # 前向计算结果(预测结果）\n",
    "        loss = criteon(logits, target)  # 计算损失\n",
    "        optimizer.zero_grad()  # 梯度清零\n",
    "        loss.backward()  # 后向传递过程\n",
    "        optimizer.step()  # 优化权重与偏差矩阵\n",
    "    end_time = time.time()\n",
    "    print(\"epoch\",epoch,' ',\"Traintime :\", end_time - start_time)\n",
    "       \n",
    "    logit = []  # 这个是验证集，可以根据验证集的结果进行调参，这里根据验证集的结果选取最优的神经网络层数与神经元数目\n",
    "    target = []\n",
    "    net.eval() # 启动测试模式\n",
    "    for data, targets in test_data:  # 输出验证集的平均误差\n",
    "        logits = net.forward(data).detach().numpy()\n",
    "        targets=targets.detach().numpy()\n",
    "        target.append(targets[0])\n",
    "        logit.append(logits[0])\n",
    "    average_loss =  criteon(torch.tensor(logit), torch.tensor(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0045)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#神经元定义\n",
    "class SpikingNeuron(nn.Module):\n",
    "    def __init__(self, threshold=1.0, decay=0.9):\n",
    "        super(SpikingNeuron, self).__init__()\n",
    "        self.threshold = threshold\n",
    "        self.decay = decay\n",
    "        self.membrane_potential = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.membrane_potential += x\n",
    "        spike = (self.membrane_potential >= self.threshold).float()\n",
    "        self.membrane_potential = self.membrane_potential * (1 - spike) * self.decay\n",
    "        return spike\n",
    "    \n",
    "#网络定义\n",
    "class SNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SNN, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_layer = SpikingNeuron()\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 0.0653, Accuracy: 0.5600\n",
      "Epoch 2/200, Loss: 0.0586, Accuracy: 0.7430\n",
      "Epoch 3/200, Loss: 0.0538, Accuracy: 0.7640\n",
      "Epoch 4/200, Loss: 0.0520, Accuracy: 0.7550\n",
      "Epoch 5/200, Loss: 0.0516, Accuracy: 0.7500\n",
      "Epoch 6/200, Loss: 0.0528, Accuracy: 0.7590\n",
      "Epoch 7/200, Loss: 0.0507, Accuracy: 0.7500\n",
      "Epoch 8/200, Loss: 0.0511, Accuracy: 0.7620\n",
      "Epoch 9/200, Loss: 0.0528, Accuracy: 0.7380\n",
      "Epoch 10/200, Loss: 0.0493, Accuracy: 0.7640\n",
      "Epoch 11/200, Loss: 0.0528, Accuracy: 0.7450\n",
      "Epoch 12/200, Loss: 0.0481, Accuracy: 0.7690\n",
      "Epoch 13/200, Loss: 0.0515, Accuracy: 0.7550\n",
      "Epoch 14/200, Loss: 0.0528, Accuracy: 0.7510\n",
      "Epoch 15/200, Loss: 0.0488, Accuracy: 0.7690\n",
      "Epoch 16/200, Loss: 0.0501, Accuracy: 0.7660\n",
      "Epoch 17/200, Loss: 0.0490, Accuracy: 0.7730\n",
      "Epoch 18/200, Loss: 0.0506, Accuracy: 0.7690\n",
      "Epoch 19/200, Loss: 0.0526, Accuracy: 0.7550\n",
      "Epoch 20/200, Loss: 0.0516, Accuracy: 0.7550\n",
      "Epoch 21/200, Loss: 0.0484, Accuracy: 0.7800\n",
      "Epoch 22/200, Loss: 0.0442, Accuracy: 0.7950\n",
      "Epoch 23/200, Loss: 0.0497, Accuracy: 0.7690\n",
      "Epoch 24/200, Loss: 0.0500, Accuracy: 0.7760\n",
      "Epoch 25/200, Loss: 0.0506, Accuracy: 0.7750\n",
      "Epoch 26/200, Loss: 0.0506, Accuracy: 0.7670\n",
      "Epoch 27/200, Loss: 0.0496, Accuracy: 0.7590\n",
      "Epoch 28/200, Loss: 0.0490, Accuracy: 0.7750\n",
      "Epoch 29/200, Loss: 0.0479, Accuracy: 0.7820\n",
      "Epoch 30/200, Loss: 0.0518, Accuracy: 0.7500\n",
      "Epoch 31/200, Loss: 0.0484, Accuracy: 0.7790\n",
      "Epoch 32/200, Loss: 0.0498, Accuracy: 0.7640\n",
      "Epoch 33/200, Loss: 0.0485, Accuracy: 0.7770\n",
      "Epoch 34/200, Loss: 0.0506, Accuracy: 0.7680\n",
      "Epoch 35/200, Loss: 0.0481, Accuracy: 0.7750\n",
      "Epoch 36/200, Loss: 0.0496, Accuracy: 0.7750\n",
      "Epoch 37/200, Loss: 0.0496, Accuracy: 0.7710\n",
      "Epoch 38/200, Loss: 0.0513, Accuracy: 0.7590\n",
      "Epoch 39/200, Loss: 0.0492, Accuracy: 0.7710\n",
      "Epoch 40/200, Loss: 0.0494, Accuracy: 0.7720\n",
      "Epoch 41/200, Loss: 0.0525, Accuracy: 0.7660\n",
      "Epoch 42/200, Loss: 0.0486, Accuracy: 0.7710\n",
      "Epoch 43/200, Loss: 0.0525, Accuracy: 0.7590\n",
      "Epoch 44/200, Loss: 0.0512, Accuracy: 0.7610\n",
      "Epoch 45/200, Loss: 0.0520, Accuracy: 0.7570\n",
      "Epoch 46/200, Loss: 0.0504, Accuracy: 0.7660\n",
      "Epoch 47/200, Loss: 0.0521, Accuracy: 0.7470\n",
      "Epoch 48/200, Loss: 0.0502, Accuracy: 0.7650\n",
      "Epoch 49/200, Loss: 0.0519, Accuracy: 0.7400\n",
      "Epoch 50/200, Loss: 0.0512, Accuracy: 0.7510\n",
      "Epoch 51/200, Loss: 0.0493, Accuracy: 0.7670\n",
      "Epoch 52/200, Loss: 0.0498, Accuracy: 0.7650\n",
      "Epoch 53/200, Loss: 0.0486, Accuracy: 0.7690\n",
      "Epoch 54/200, Loss: 0.0485, Accuracy: 0.7690\n",
      "Epoch 55/200, Loss: 0.0517, Accuracy: 0.7500\n",
      "Epoch 56/200, Loss: 0.0492, Accuracy: 0.7800\n",
      "Epoch 57/200, Loss: 0.0483, Accuracy: 0.7790\n",
      "Epoch 58/200, Loss: 0.0474, Accuracy: 0.7780\n",
      "Epoch 59/200, Loss: 0.0487, Accuracy: 0.7680\n",
      "Epoch 60/200, Loss: 0.0499, Accuracy: 0.7720\n",
      "Epoch 61/200, Loss: 0.0499, Accuracy: 0.7780\n",
      "Epoch 62/200, Loss: 0.0515, Accuracy: 0.7520\n",
      "Epoch 63/200, Loss: 0.0504, Accuracy: 0.7750\n",
      "Epoch 64/200, Loss: 0.0499, Accuracy: 0.7640\n",
      "Epoch 65/200, Loss: 0.0496, Accuracy: 0.7660\n",
      "Epoch 66/200, Loss: 0.0522, Accuracy: 0.7580\n",
      "Epoch 67/200, Loss: 0.0516, Accuracy: 0.7490\n",
      "Epoch 68/200, Loss: 0.0487, Accuracy: 0.7760\n",
      "Epoch 69/200, Loss: 0.0509, Accuracy: 0.7580\n",
      "Epoch 70/200, Loss: 0.0492, Accuracy: 0.7730\n",
      "Epoch 71/200, Loss: 0.0487, Accuracy: 0.7870\n",
      "Epoch 72/200, Loss: 0.0482, Accuracy: 0.7930\n",
      "Epoch 73/200, Loss: 0.0514, Accuracy: 0.7590\n",
      "Epoch 74/200, Loss: 0.0465, Accuracy: 0.7950\n",
      "Epoch 75/200, Loss: 0.0525, Accuracy: 0.7440\n",
      "Epoch 76/200, Loss: 0.0500, Accuracy: 0.7760\n",
      "Epoch 77/200, Loss: 0.0469, Accuracy: 0.7840\n",
      "Epoch 78/200, Loss: 0.0497, Accuracy: 0.7650\n",
      "Epoch 79/200, Loss: 0.0513, Accuracy: 0.7510\n",
      "Epoch 80/200, Loss: 0.0509, Accuracy: 0.7620\n",
      "Epoch 81/200, Loss: 0.0507, Accuracy: 0.7610\n",
      "Epoch 82/200, Loss: 0.0495, Accuracy: 0.7750\n",
      "Epoch 83/200, Loss: 0.0516, Accuracy: 0.7740\n",
      "Epoch 84/200, Loss: 0.0480, Accuracy: 0.7770\n",
      "Epoch 85/200, Loss: 0.0536, Accuracy: 0.7570\n",
      "Epoch 86/200, Loss: 0.0516, Accuracy: 0.7570\n",
      "Epoch 87/200, Loss: 0.0500, Accuracy: 0.7690\n",
      "Epoch 88/200, Loss: 0.0503, Accuracy: 0.7770\n",
      "Epoch 89/200, Loss: 0.0503, Accuracy: 0.7570\n",
      "Epoch 90/200, Loss: 0.0508, Accuracy: 0.7670\n",
      "Epoch 91/200, Loss: 0.0500, Accuracy: 0.7730\n",
      "Epoch 92/200, Loss: 0.0500, Accuracy: 0.7610\n",
      "Epoch 93/200, Loss: 0.0482, Accuracy: 0.7660\n",
      "Epoch 94/200, Loss: 0.0495, Accuracy: 0.7750\n",
      "Epoch 95/200, Loss: 0.0535, Accuracy: 0.7460\n",
      "Epoch 96/200, Loss: 0.0519, Accuracy: 0.7540\n",
      "Epoch 97/200, Loss: 0.0513, Accuracy: 0.7590\n",
      "Epoch 98/200, Loss: 0.0522, Accuracy: 0.7610\n",
      "Epoch 99/200, Loss: 0.0486, Accuracy: 0.7790\n",
      "Epoch 100/200, Loss: 0.0475, Accuracy: 0.7820\n",
      "Epoch 101/200, Loss: 0.0510, Accuracy: 0.7620\n",
      "Epoch 102/200, Loss: 0.0509, Accuracy: 0.7630\n",
      "Epoch 103/200, Loss: 0.0522, Accuracy: 0.7570\n",
      "Epoch 104/200, Loss: 0.0529, Accuracy: 0.7330\n",
      "Epoch 105/200, Loss: 0.0499, Accuracy: 0.7730\n",
      "Epoch 106/200, Loss: 0.0489, Accuracy: 0.7800\n",
      "Epoch 107/200, Loss: 0.0487, Accuracy: 0.7680\n",
      "Epoch 108/200, Loss: 0.0517, Accuracy: 0.7470\n",
      "Epoch 109/200, Loss: 0.0497, Accuracy: 0.7800\n",
      "Epoch 110/200, Loss: 0.0478, Accuracy: 0.7790\n",
      "Epoch 111/200, Loss: 0.0473, Accuracy: 0.7780\n",
      "Epoch 112/200, Loss: 0.0513, Accuracy: 0.7710\n",
      "Epoch 113/200, Loss: 0.0492, Accuracy: 0.7770\n",
      "Epoch 114/200, Loss: 0.0501, Accuracy: 0.7720\n",
      "Epoch 115/200, Loss: 0.0479, Accuracy: 0.7850\n",
      "Epoch 116/200, Loss: 0.0510, Accuracy: 0.7600\n",
      "Epoch 117/200, Loss: 0.0502, Accuracy: 0.7690\n",
      "Epoch 118/200, Loss: 0.0512, Accuracy: 0.7510\n",
      "Epoch 119/200, Loss: 0.0492, Accuracy: 0.7730\n",
      "Epoch 120/200, Loss: 0.0516, Accuracy: 0.7580\n",
      "Epoch 121/200, Loss: 0.0498, Accuracy: 0.7630\n",
      "Epoch 122/200, Loss: 0.0505, Accuracy: 0.7600\n",
      "Epoch 123/200, Loss: 0.0495, Accuracy: 0.7720\n",
      "Epoch 124/200, Loss: 0.0489, Accuracy: 0.7640\n",
      "Epoch 125/200, Loss: 0.0508, Accuracy: 0.7650\n",
      "Epoch 126/200, Loss: 0.0498, Accuracy: 0.7750\n",
      "Epoch 127/200, Loss: 0.0485, Accuracy: 0.7800\n",
      "Epoch 128/200, Loss: 0.0500, Accuracy: 0.7730\n",
      "Epoch 129/200, Loss: 0.0490, Accuracy: 0.7770\n",
      "Epoch 130/200, Loss: 0.0516, Accuracy: 0.7630\n",
      "Epoch 131/200, Loss: 0.0504, Accuracy: 0.7610\n",
      "Epoch 132/200, Loss: 0.0500, Accuracy: 0.7670\n",
      "Epoch 133/200, Loss: 0.0519, Accuracy: 0.7570\n",
      "Epoch 134/200, Loss: 0.0523, Accuracy: 0.7630\n",
      "Epoch 135/200, Loss: 0.0493, Accuracy: 0.7780\n",
      "Epoch 136/200, Loss: 0.0490, Accuracy: 0.7800\n",
      "Epoch 137/200, Loss: 0.0507, Accuracy: 0.7620\n",
      "Epoch 138/200, Loss: 0.0485, Accuracy: 0.7840\n",
      "Epoch 139/200, Loss: 0.0498, Accuracy: 0.7800\n",
      "Epoch 140/200, Loss: 0.0500, Accuracy: 0.7680\n",
      "Epoch 141/200, Loss: 0.0513, Accuracy: 0.7550\n",
      "Epoch 142/200, Loss: 0.0525, Accuracy: 0.7480\n",
      "Epoch 143/200, Loss: 0.0496, Accuracy: 0.7690\n",
      "Epoch 144/200, Loss: 0.0492, Accuracy: 0.7790\n",
      "Epoch 145/200, Loss: 0.0497, Accuracy: 0.7780\n",
      "Epoch 146/200, Loss: 0.0487, Accuracy: 0.7650\n",
      "Epoch 147/200, Loss: 0.0474, Accuracy: 0.7820\n",
      "Epoch 148/200, Loss: 0.0519, Accuracy: 0.7590\n",
      "Epoch 149/200, Loss: 0.0459, Accuracy: 0.7920\n",
      "Epoch 150/200, Loss: 0.0503, Accuracy: 0.7680\n",
      "Epoch 151/200, Loss: 0.0520, Accuracy: 0.7420\n",
      "Epoch 152/200, Loss: 0.0513, Accuracy: 0.7490\n",
      "Epoch 153/200, Loss: 0.0488, Accuracy: 0.7640\n",
      "Epoch 154/200, Loss: 0.0491, Accuracy: 0.7710\n",
      "Epoch 155/200, Loss: 0.0502, Accuracy: 0.7620\n",
      "Epoch 156/200, Loss: 0.0502, Accuracy: 0.7520\n",
      "Epoch 157/200, Loss: 0.0515, Accuracy: 0.7650\n",
      "Epoch 158/200, Loss: 0.0535, Accuracy: 0.7430\n",
      "Epoch 159/200, Loss: 0.0521, Accuracy: 0.7530\n",
      "Epoch 160/200, Loss: 0.0519, Accuracy: 0.7550\n",
      "Epoch 161/200, Loss: 0.0529, Accuracy: 0.7470\n",
      "Epoch 162/200, Loss: 0.0509, Accuracy: 0.7640\n",
      "Epoch 163/200, Loss: 0.0529, Accuracy: 0.7490\n",
      "Epoch 164/200, Loss: 0.0499, Accuracy: 0.7820\n",
      "Epoch 165/200, Loss: 0.0517, Accuracy: 0.7520\n",
      "Epoch 166/200, Loss: 0.0492, Accuracy: 0.7680\n",
      "Epoch 167/200, Loss: 0.0528, Accuracy: 0.7430\n",
      "Epoch 168/200, Loss: 0.0518, Accuracy: 0.7540\n",
      "Epoch 169/200, Loss: 0.0532, Accuracy: 0.7380\n",
      "Epoch 170/200, Loss: 0.0490, Accuracy: 0.7690\n",
      "Epoch 171/200, Loss: 0.0499, Accuracy: 0.7750\n",
      "Epoch 172/200, Loss: 0.0507, Accuracy: 0.7650\n",
      "Epoch 173/200, Loss: 0.0491, Accuracy: 0.7830\n",
      "Epoch 174/200, Loss: 0.0501, Accuracy: 0.7780\n",
      "Epoch 175/200, Loss: 0.0518, Accuracy: 0.7460\n",
      "Epoch 176/200, Loss: 0.0501, Accuracy: 0.7660\n",
      "Epoch 177/200, Loss: 0.0500, Accuracy: 0.7670\n",
      "Epoch 178/200, Loss: 0.0486, Accuracy: 0.7850\n",
      "Epoch 179/200, Loss: 0.0504, Accuracy: 0.7620\n",
      "Epoch 180/200, Loss: 0.0508, Accuracy: 0.7680\n",
      "Epoch 181/200, Loss: 0.0494, Accuracy: 0.7640\n",
      "Epoch 182/200, Loss: 0.0523, Accuracy: 0.7550\n",
      "Epoch 183/200, Loss: 0.0493, Accuracy: 0.7730\n",
      "Epoch 184/200, Loss: 0.0525, Accuracy: 0.7630\n",
      "Epoch 185/200, Loss: 0.0480, Accuracy: 0.7870\n",
      "Epoch 186/200, Loss: 0.0501, Accuracy: 0.7710\n",
      "Epoch 187/200, Loss: 0.0512, Accuracy: 0.7510\n",
      "Epoch 188/200, Loss: 0.0495, Accuracy: 0.7650\n",
      "Epoch 189/200, Loss: 0.0513, Accuracy: 0.7690\n",
      "Epoch 190/200, Loss: 0.0505, Accuracy: 0.7570\n",
      "Epoch 191/200, Loss: 0.0494, Accuracy: 0.7600\n",
      "Epoch 192/200, Loss: 0.0483, Accuracy: 0.7790\n",
      "Epoch 193/200, Loss: 0.0503, Accuracy: 0.7680\n",
      "Epoch 194/200, Loss: 0.0499, Accuracy: 0.7740\n",
      "Epoch 195/200, Loss: 0.0505, Accuracy: 0.7610\n",
      "Epoch 196/200, Loss: 0.0478, Accuracy: 0.7790\n",
      "Epoch 197/200, Loss: 0.0516, Accuracy: 0.7510\n",
      "Epoch 198/200, Loss: 0.0495, Accuracy: 0.7650\n",
      "Epoch 199/200, Loss: 0.0509, Accuracy: 0.7590\n",
      "Epoch 200/200, Loss: 0.0497, Accuracy: 0.7700\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "# 生成数据样例\n",
    "X = torch.randn(1000, 2)\n",
    "y = (X[:, 0] + X[:, 1] > 0).float()\n",
    "\n",
    "# 创建数据加载器\n",
    "dataset = data.TensorDataset(X, y)\n",
    "data_loader = data.DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "model = SNN(input_size=2, hidden_size=10, output_size=1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X_batch, y_batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs.view(-1), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        correct += ((outputs.view(-1) > 0) == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / total:.4f}, Accuracy: {correct / total:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\26921\\AppData\\Local\\Temp\\ipykernel_25508\\1689855637.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model.forward(torch.tensor(X[0],dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1754],\n",
       "        [ 1.1754],\n",
       "        [ 0.2077],\n",
       "        [ 1.6957],\n",
       "        [ 1.1754],\n",
       "        [ 0.2077],\n",
       "        [ 1.6957],\n",
       "        [-1.7945],\n",
       "        [ 1.1754],\n",
       "        [ 0.2077]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(torch.tensor(X[0],dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6229, Test Accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "# 生成测试数据\n",
    "X_test = torch.randn(10, 2)\n",
    "y_test = (X_test[:, 0] + X_test[:, 1] > 0).float()\n",
    "\n",
    "# 测试模型\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    test_loss = criterion(outputs.view(-1), y_test)\n",
    "    test_accuracy = ((outputs.view(-1) > 0) == y_test).sum().item() / y_test.size(0)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaky neuron model, overriding the backward pass with a custom function\n",
    "class LeakySurrogate(nn.Module):\n",
    "  def __init__(self, beta, threshold=1.0):\n",
    "      super(LeakySurrogate, self).__init__()\n",
    "\n",
    "      # initialize decay rate beta and threshold\n",
    "      self.beta = beta\n",
    "      self.threshold = threshold\n",
    "      self.spike_op = self.SpikeOperator.apply\n",
    "  \n",
    "  # the forward function is called each time we call Leaky\n",
    "  def forward(self, input_, mem):\n",
    "    spk = self.spike_op((mem-self.threshold))  # call the Heaviside function\n",
    "    reset = (spk * self.threshold).detach() # removes spike_op gradient from reset\n",
    "    mem = self.beta * mem + input_ - reset # Eq (1)\n",
    "    return spk, mem\n",
    "\n",
    "  # Forward pass: Heaviside function\n",
    "  # Backward pass: Override Dirac Delta with the Spike itself\n",
    "  @staticmethod\n",
    "  class SpikeOperator(torch.autograd.Function):\n",
    "      @staticmethod\n",
    "      def forward(ctx, mem):\n",
    "          spk = (mem > 0).float() # Heaviside on the forward pass: Eq(2)\n",
    "          ctx.save_for_backward(spk)  # store the spike for use in the backward pass\n",
    "          return spk\n",
    "\n",
    "      @staticmethod\n",
    "      def backward(ctx, grad_output):\n",
    "          (spk,) = ctx.saved_tensors  # retrieve the spike \n",
    "          grad = grad_output * spk # scale the gradient by the spike: 1/0\n",
    "          return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastSigmoid(torch.autograd.Function):\n",
    "    @staticmethod \n",
    "    def forward(ctx, input_, slope=25): \n",
    "        ctx.save_for_backward(input_) \n",
    "        ctx.slope = slope \n",
    "        out = (input_ > 0).float() \n",
    "        return out \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output): \n",
    "        (input_,) = ctx.saved_tensors \n",
    "        grad_input = grad_output.clone() \n",
    "        grad = grad_input / (ctx.slope * torch.abs(input_) + 1.0) ** 2 \n",
    "        return grad, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hybrid_neuron(nn.Module): \n",
    "    def __init__(self, input_shape, output_shape, T, batch_size): \n",
    "        super(hybrid_neuron, self).__init__() \n",
    "        self.batch_size = batch_size \n",
    "        self.input_shape = input_shape \n",
    "        self.output_shape = output_shape \n",
    "        self.MLP = MLP(input_shape=self.input_shape, \n",
    "                        output_shape=self.output_shape).float().to() \n",
    "        self.T = T # num steps        \n",
    "        self.sigmoid = FastSigmoid()\n",
    "        self.alpha = torch.autograd.Variable(torch.tensor(0.), \n",
    "                                             requires_grad=True).to() \n",
    "        # Alpha, Eta, Beta       \n",
    "        self.eta = torch.autograd.Variable(torch.tensor(0.), requires_grad=True).to() \n",
    "        self.beta = torch.autograd.Variable(torch.tensor(0.), requires_grad=True).to() \n",
    "        self.tao_u = torch.tensor(5).to() \n",
    "        # membrane time constant        \n",
    "        self.tao_w = torch.tensor(1).to() \n",
    "        # synaptic constant         \n",
    "        self.dt = torch.tensor(1).to() \n",
    "        self.s = torch.zeros(self.output_shape, self.batch_size).to() \n",
    "        self.u = torch.zeros(self.output_shape, self.batch_size).to() \n",
    "        self.k = self.dt / self.tao_u \n",
    "        self.V_th = torch.tensor(0.2).to() \n",
    "        self.P = torch.autograd.Variable(torch.zeros((self.output_shape, self.input_shape)), requires_grad=True).to() \n",
    "\n",
    "    def forward(self, s_batch): \n",
    "            spk = [] \n",
    "            for m in range(self.T): \n",
    "                 # for s in s_batch:            \n",
    "                s = s_batch \n",
    "                decay_1 = torch.tensor(-m) / self.tao_w \n",
    "                self.u = torch.mul((1 - self.s) * (1 - self.k), self.u) + \\\n",
    "                    self.k * ((self.MLP(s) * decay_1.exp()).T + self.alpha * torch.matmul(self.P, s.T)) \n",
    "                \n",
    "                self.P = self.P * (-self.dt / self.tao_w).exp() + \\\n",
    "                    self.eta * torch.matmul(self.sigmoid(self.u) + self.beta,s)\n",
    "                \n",
    "                self.s = self.sigmoid(self.u - self.V_th) \n",
    "                spk.append(self.s.T) \n",
    "                self.reset()\n",
    "                return torch.stack(spk) \n",
    "    \n",
    "    def reset(self): \n",
    "        self.s = torch.zeros(self.output_shape, self.batch_size).to() \n",
    "        self.u = torch.zeros(self.output_shape, self.batch_size).to()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
