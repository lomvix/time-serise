{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:342: SyntaxWarning: invalid escape sequence '\\E'\n",
      "<>:342: SyntaxWarning: invalid escape sequence '\\E'\n",
      "C:\\Users\\26921\\AppData\\Local\\Temp\\ipykernel_28096\\89944053.py:342: SyntaxWarning: invalid escape sequence '\\E'\n",
      "  data = pd.read_csv('data\\ETTh.csv', usecols=[1], engine='python')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         HUFL\n",
      "0       5.827\n",
      "1       5.693\n",
      "2       5.157\n",
      "3       5.090\n",
      "4       5.358\n",
      "...       ...\n",
      "17415  -1.674\n",
      "17416  -5.492\n",
      "17417   2.813\n",
      "17418   9.243\n",
      "17419  10.114\n",
      "\n",
      "[17420 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\26921\\AppData\\Local\\Temp\\ipykernel_28096\\89944053.py:362: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\b\\abs_8f7uhuge1i\\croot\\pytorch-select_1717607507421\\work\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  return torch.Tensor(dataX), torch.Tensor(dataY)\n",
      "Training xLSTM:  10%|â–ˆ         | 2/20 [01:11<10:47, 35.98s/it]\n",
      "C:\\Users\\26921\\AppData\\Local\\Temp\\ipykernel_28096\\89944053.py:342: SyntaxWarning: invalid escape sequence '\\E'\n",
      "  data = pd.read_csv('data\\ETTh.csv', usecols=[1], engine='python')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 418\u001b[0m\n\u001b[0;32m    416\u001b[0m all_train_losses \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 418\u001b[0m     trained_models[model_name], all_train_losses[model_name] \u001b[38;5;241m=\u001b[39m train_model(model, model_name)\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# Plot losses for each model\u001b[39;00m\n\u001b[0;32m    421\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
      "Cell \u001b[1;32mIn[1], line 404\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, model_name, epochs, learning_rate)\u001b[0m\n\u001b[0;32m    402\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m    403\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m--> 404\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    405\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    407\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    527\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[0;32m    268\u001b[0m     tensors,\n\u001b[0;32m    269\u001b[0m     grad_tensors_,\n\u001b[0;32m    270\u001b[0m     retain_graph,\n\u001b[0;32m    271\u001b[0m     create_graph,\n\u001b[0;32m    272\u001b[0m     inputs,\n\u001b[0;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CausalConv1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, **kwargs):\n",
    "        super(CausalConv1D, self).__init__()\n",
    "        self.padding = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=self.padding, dilation=dilation, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x[:, :, :-self.padding]\n",
    "\n",
    "\n",
    "class BlockDiagonal(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_blocks):\n",
    "        super(BlockDiagonal, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        assert out_features % num_blocks == 0\n",
    "\n",
    "        block_out_features = out_features // num_blocks\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Linear(in_features, block_out_features)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = [block(x) for block in self.blocks]\n",
    "        x = torch.cat(x, dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class sLSTMBlock(nn.Module):\n",
    "    def __init__(self, input_size, head_size, num_heads, proj_factor=4 / 3):\n",
    "        super(sLSTMBlock, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.head_size = head_size\n",
    "        self.hidden_size = head_size * num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.proj_factor = proj_factor\n",
    "\n",
    "        assert proj_factor >0\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(input_size)\n",
    "        self.causal_conv = CausalConv1D(1, 1, 4)\n",
    "\n",
    "        self.Wz = BlockDiagonal(input_size, self.hidden_size, num_heads)\n",
    "        self.Wi = BlockDiagonal(input_size, self.hidden_size, num_heads)\n",
    "        self.Wf = BlockDiagonal(input_size, self.hidden_size, num_heads)\n",
    "        self.Wo = BlockDiagonal(input_size, self.hidden_size, num_heads)\n",
    "\n",
    "        self.Rz = BlockDiagonal(self.hidden_size, self.hidden_size, num_heads)\n",
    "        self.Ri = BlockDiagonal(self.hidden_size, self.hidden_size, num_heads)\n",
    "        self.Rf = BlockDiagonal(self.hidden_size, self.hidden_size, num_heads)\n",
    "        self.Ro = BlockDiagonal(self.hidden_size, self.hidden_size, num_heads)\n",
    "\n",
    "        self.group_norm = nn.GroupNorm(num_heads, self.hidden_size)\n",
    "\n",
    "        self.up_proj_left = nn.Linear(self.hidden_size, int(self.hidden_size * proj_factor))\n",
    "        self.up_proj_right = nn.Linear(self.hidden_size, int(self.hidden_size * proj_factor))\n",
    "        self.down_proj = nn.Linear(int(self.hidden_size * proj_factor), input_size)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        assert x.size(-1) == self.input_size\n",
    "        h_prev, c_prev, n_prev, m_prev = prev_state\n",
    "\n",
    "        h_prev = h_prev.to(x.device)\n",
    "        c_prev = c_prev.to(x.device)\n",
    "        n_prev = n_prev.to(x.device)\n",
    "        m_prev = m_prev.to(x.device)\n",
    "\n",
    "        x_norm = self.layer_norm(x)\n",
    "        x_conv = F.silu(self.causal_conv(x_norm.unsqueeze(1)).squeeze(1))\n",
    "\n",
    "        z = torch.tanh(self.Wz(x_norm) + self.Rz(h_prev))\n",
    "        o = torch.sigmoid(self.Wo(x_norm) + self.Ro(h_prev))\n",
    "        i_tilde = self.Wi(x_conv) + self.Ri(h_prev)\n",
    "        f_tilde = self.Wf(x_conv) + self.Rf(h_prev)\n",
    "\n",
    "        m_t = torch.max(f_tilde + m_prev, i_tilde)\n",
    "        i = torch.exp(i_tilde - m_t)\n",
    "        f = torch.exp(f_tilde + m_prev - m_t)\n",
    "\n",
    "        c_t = f * c_prev + i * z\n",
    "        n_t = f * n_prev + i\n",
    "        h_t = o * c_t / n_t\n",
    "\n",
    "        output = h_t\n",
    "        output_norm = self.group_norm(output)\n",
    "        output_left = self.up_proj_left(output_norm)\n",
    "        output_right = self.up_proj_right(output_norm)\n",
    "        output_gated = F.gelu(output_right)\n",
    "        output = output_left * output_gated\n",
    "        output = self.down_proj(output)\n",
    "        final_output = output + x\n",
    "\n",
    "        return final_output, (h_t, c_t, n_t, m_t)\n",
    "\n",
    "\n",
    "class sLSTM(nn.Module):\n",
    "    # TODO: Add bias, dropout, bidirectional\n",
    "    def __init__(self, input_size, head_size, num_heads, num_layers=1, batch_first=False, proj_factor=4 / 3):\n",
    "        super(sLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.head_size = head_size\n",
    "        self.hidden_size = head_size * num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.proj_factor_slstm = proj_factor\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [sLSTMBlock(input_size, head_size, num_heads, proj_factor) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, state=None):\n",
    "        assert x.ndim == 3\n",
    "        if self.batch_first: x = x.transpose(0, 1)\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "\n",
    "        if state is not None:\n",
    "            state = torch.stack(list(state)).to(x.device)\n",
    "            assert state.ndim == 4\n",
    "            num_hidden, state_num_layers, state_batch_size, state_input_size = state.size()\n",
    "            assert num_hidden == 4\n",
    "            assert state_num_layers == self.num_layers\n",
    "            assert state_batch_size == batch_size\n",
    "            assert state_input_size == self.input_size\n",
    "            state = state.transpose(0, 1)\n",
    "        else:\n",
    "            state = torch.zeros(self.num_layers, 4, batch_size, self.hidden_size, device=x.device)\n",
    "\n",
    "        output = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[t]\n",
    "            for layer in range(self.num_layers):\n",
    "                x_t, state_tuple = self.layers[layer](x_t, tuple(state[layer].clone()))\n",
    "                state[layer] = torch.stack(list(state_tuple))\n",
    "            output.append(x_t)\n",
    "\n",
    "        output = torch.stack(output)\n",
    "        if self.batch_first:\n",
    "            output = output.transpose(0, 1)\n",
    "        state = tuple(state.transpose(0, 1))\n",
    "        return output, state\n",
    "\n",
    "\n",
    "class mLSTMBlock(nn.Module):\n",
    "    def __init__(self, input_size, head_size, num_heads, proj_factor=2):\n",
    "        super(mLSTMBlock, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.head_size = head_size\n",
    "        self.hidden_size = head_size * num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.proj_factor = proj_factor\n",
    "\n",
    "        assert proj_factor >0\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(input_size)\n",
    "        self.up_proj_left = nn.Linear(input_size, int(input_size * proj_factor))\n",
    "        self.up_proj_right = nn.Linear(input_size, self.hidden_size)\n",
    "        self.down_proj = nn.Linear(self.hidden_size, input_size)\n",
    "\n",
    "        self.causal_conv = CausalConv1D(1, 1, 4)\n",
    "        self.skip_connection = nn.Linear(int(input_size * proj_factor), self.hidden_size)\n",
    "\n",
    "        self.Wq = BlockDiagonal(int(input_size * proj_factor), self.hidden_size, num_heads)\n",
    "        self.Wk = BlockDiagonal(int(input_size * proj_factor), self.hidden_size, num_heads)\n",
    "        self.Wv = BlockDiagonal(int(input_size * proj_factor), self.hidden_size, num_heads)\n",
    "        self.Wi = nn.Linear(int(input_size * proj_factor), self.hidden_size)\n",
    "        self.Wf = nn.Linear(int(input_size * proj_factor), self.hidden_size)\n",
    "        self.Wo = nn.Linear(int(input_size * proj_factor), self.hidden_size)\n",
    "\n",
    "        self.group_norm = nn.GroupNorm(num_heads, self.hidden_size)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        h_prev, c_prev, n_prev, m_prev = prev_state\n",
    "\n",
    "        h_prev = h_prev.to(x.device)\n",
    "        c_prev = c_prev.to(x.device)\n",
    "        n_prev = n_prev.to(x.device)\n",
    "        m_prev = m_prev.to(x.device)\n",
    "\n",
    "        assert x.size(-1) == self.input_size\n",
    "        x_norm = self.layer_norm(x)\n",
    "        x_up_left = self.up_proj_left(x_norm)\n",
    "        x_up_right = self.up_proj_right(x_norm)\n",
    "\n",
    "        x_conv = F.silu(self.causal_conv(x_up_left.unsqueeze(1)).squeeze(1))\n",
    "        x_skip = self.skip_connection(x_conv)\n",
    "\n",
    "        q = self.Wq(x_conv)\n",
    "        k = self.Wk(x_conv) / (self.head_size ** 0.5)\n",
    "        v = self.Wv(x_up_left)\n",
    "\n",
    "        i_tilde = self.Wi(x_conv)\n",
    "        f_tilde = self.Wf(x_conv)\n",
    "        o = torch.sigmoid(self.Wo(x_up_left))\n",
    "\n",
    "        m_t = torch.max(f_tilde + m_prev, i_tilde)\n",
    "        i = torch.exp(i_tilde - m_t)\n",
    "        f = torch.exp(f_tilde + m_prev - m_t)\n",
    "\n",
    "        c_t = f * c_prev + i * (v * k)  # v @ k.T\n",
    "        n_t = f * n_prev + i * k\n",
    "        h_t = o * (c_t * q) / torch.max(torch.abs(n_t.T @ q), 1)[0]  # o * (c @ q) / max{|n.T @ q|, 1}\n",
    "\n",
    "        output = h_t\n",
    "        output_norm = self.group_norm(output)\n",
    "        output = output_norm + x_skip\n",
    "        output = output * F.silu(x_up_right)\n",
    "        output = self.down_proj(output)\n",
    "        final_output = output + x\n",
    "\n",
    "        return final_output, (h_t, c_t, n_t, m_t)\n",
    "\n",
    "\n",
    "class mLSTM(nn.Module):\n",
    "    # TODO: Add bias, dropout, bidirectional\n",
    "    def __init__(self, input_size, head_size, num_heads, num_layers=1, batch_first=False, proj_factor=2):\n",
    "        super(mLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.head_size = head_size\n",
    "        self.hidden_size = head_size * num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.proj_factor_slstm = proj_factor\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [mLSTMBlock(input_size, head_size, num_heads, proj_factor) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, state=None):\n",
    "        assert x.ndim == 3\n",
    "        if self.batch_first: x = x.transpose(0, 1)\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "\n",
    "        if state is not None:\n",
    "            state = torch.stack(list(state)).to(x.device)\n",
    "            assert state.ndim == 4\n",
    "            num_hidden, state_num_layers, state_batch_size, state_input_size = state.size()\n",
    "            assert num_hidden == 4\n",
    "            assert state_num_layers == self.num_layers\n",
    "            assert state_batch_size == batch_size\n",
    "            assert state_input_size == self.input_size\n",
    "            state = state.transpose(0, 1)\n",
    "        else:\n",
    "            state = torch.zeros(self.num_layers, 4, batch_size, self.hidden_size, device=x.device)\n",
    "\n",
    "        output = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[t]\n",
    "            for layer in range(self.num_layers):\n",
    "                x_t, state_tuple = self.layers[layer](x_t, tuple(state[layer].clone()))\n",
    "                state[layer] = torch.stack(list(state_tuple))\n",
    "            output.append(x_t)\n",
    "\n",
    "        output = torch.stack(output)\n",
    "        if self.batch_first:\n",
    "            output = output.transpose(0, 1)\n",
    "        state = tuple(state.transpose(0, 1))\n",
    "        return output, state\n",
    "\n",
    "\n",
    "class xLSTM(nn.Module):\n",
    "    # TODO: Add bias, dropout, bidirectional\n",
    "    def __init__(self, input_size, head_size, num_heads, layers, batch_first=False, proj_factor_slstm=4 / 3,\n",
    "                 proj_factor_mlstm=2):\n",
    "        super(xLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.head_size = head_size\n",
    "        self.hidden_size = head_size * num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.layers = layers\n",
    "        self.num_layers = len(layers)\n",
    "        self.batch_first = batch_first\n",
    "        self.proj_factor_slstm = proj_factor_slstm\n",
    "        self.proj_factor_mlstm = proj_factor_mlstm\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for layer_type in layers:\n",
    "            if layer_type == 's':\n",
    "                layer = sLSTMBlock(input_size, head_size, num_heads, proj_factor_slstm)\n",
    "            elif layer_type == 'm':\n",
    "                layer = mLSTMBlock(input_size, head_size, num_heads, proj_factor_mlstm)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid layer type: {layer_type}. Choose 's' for sLSTM or 'm' for mLSTM.\")\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x, state=None):\n",
    "        assert x.ndim == 3\n",
    "        if self.batch_first: x = x.transpose(0, 1)\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "\n",
    "        if state is not None:\n",
    "            state = torch.stack(list(state)).to(x.device)\n",
    "            assert state.ndim == 4\n",
    "            num_hidden, state_num_layers, state_batch_size, state_input_size = state.size()\n",
    "            assert num_hidden == 4\n",
    "            assert state_num_layers == self.num_layers\n",
    "            assert state_batch_size == batch_size\n",
    "            assert state_input_size == self.input_size\n",
    "            state = state.transpose(0, 1)\n",
    "        else:\n",
    "            state = torch.zeros(self.num_layers, 4, batch_size, self.hidden_size, device=x.device)\n",
    "\n",
    "        output = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[t]\n",
    "            for layer in range(self.num_layers):\n",
    "                x_t, state_tuple = self.layers[layer](x_t, tuple(state[layer].clone()))\n",
    "                state[layer] = torch.stack(list(state_tuple))\n",
    "            output.append(x_t)\n",
    "\n",
    "        output = torch.stack(output)\n",
    "        if self.batch_first:\n",
    "            output = output.transpose(0, 1)\n",
    "        state = tuple(state.transpose(0, 1))\n",
    "        return output, state\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# Dataset hyperparameters\n",
    "batch_size = 32\n",
    "seq_len = 8\n",
    "\n",
    "# Load and preprocess the AirPassengers dataset\n",
    "data = pd.read_csv('data\\ETTh.csv', usecols=[1], engine='python')\n",
    "print(data)\n",
    "dataset = data.values.astype('float32')\n",
    "\n",
    "# Normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\n",
    "\n",
    "\n",
    "def create_dataset(dataset, seq_len=8):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - seq_len):\n",
    "        a = dataset[i:(i + seq_len - 1)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + seq_len - 1])\n",
    "    return torch.Tensor(dataX), torch.Tensor(dataY)\n",
    "\n",
    "\n",
    "trainX, trainY = create_dataset(train, seq_len)\n",
    "testX, testY = create_dataset(test, seq_len)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(trainX, trainY)\n",
    "test_dataset = TensorDataset(testX, testY)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model hyperparameters\n",
    "input_size = 1\n",
    "head_size = 32\n",
    "num_heads = 2\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"xLSTM\": xLSTM(input_size, head_size, num_heads, batch_first=True, layers='msm'),\n",
    "    \"LSTM\": nn.LSTM(input_size, head_size, batch_first=True, proj_size=input_size),\n",
    "    \"sLSTM\": sLSTM(input_size, head_size, num_heads, batch_first=True),\n",
    "    \"mLSTM\": mLSTM(input_size, head_size, num_heads, batch_first=True)\n",
    "}\n",
    "\n",
    "\n",
    "# Training process\n",
    "def train_model(model, model_name, epochs=20, learning_rate=0.01):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    for epoch in tqdm(range(epochs), desc=f'Training {model_name}'):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs, _ = model(inputs)\n",
    "            outputs = outputs[:, -1, :]\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "    return model, train_losses\n",
    "\n",
    "\n",
    "# Train the models\n",
    "trained_models = {}\n",
    "all_train_losses = {}\n",
    "for model_name, model in models.items():\n",
    "    trained_models[model_name], all_train_losses[model_name] = train_model(model, model_name)\n",
    "\n",
    "# Plot losses for each model\n",
    "plt.figure()\n",
    "for model_name, train_losses in all_train_losses.items():\n",
    "    plt.plot(train_losses, label=model_name)\n",
    "\n",
    "# Plot all model losses compared\n",
    "plt.title('Training Losses for all Models')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Evaluate models on test data\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            outputs, _ = model(inputs)\n",
    "            predictions.extend(outputs[:, -1, :].numpy())\n",
    "    return predictions\n",
    "\n",
    "\n",
    "test_predictions = {}\n",
    "for model_name, model in trained_models.items():\n",
    "    test_predictions[model_name] = evaluate_model(model, test_loader)\n",
    "final_dict = {}\n",
    "# Plot predictions for each model\n",
    "for model_name, preds in test_predictions.items():\n",
    "    # Inverse transform the predictions and actual values\n",
    "    preds = scaler.inverse_transform(np.array(preds).reshape(-1, 1))\n",
    "    actual = scaler.inverse_transform(testY.numpy().reshape(-1, 1))\n",
    "    print(r2_score(actual, preds))\n",
    "    #print(preds)\n",
    "    final_dict[model_name] = preds\n",
    "    plt.figure()\n",
    "    plt.plot(actual, label='Actual')\n",
    "    plt.plot(preds, label=model_name + ' Predictions')\n",
    "    plt.title(f'{model_name} Predictions vs Actual')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot all model predictions compared\n",
    "plt.figure()\n",
    "plt.plot(actual, label='Actual')\n",
    "for model_name, preds in test_predictions.items():\n",
    "    # Inverse transform the predictions\n",
    "    preds = scaler.inverse_transform(np.array(preds).reshape(-1, 1))\n",
    "    plt.plot(preds, label=model_name + ' Predictions')\n",
    "\n",
    "plt.title('All Models Predictions vs Actual')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i in final_dict:\n",
    "    print(f\"{i} : {r2_score(actual, final_dict[i])}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
