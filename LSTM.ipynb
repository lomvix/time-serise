{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/3号机组抽水态健康样本.csv\",encoding=\"GBK\")\n",
    "X=data.iloc[:,13:]\n",
    "y=data.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_len = 5\n",
    "serise = np.array([y[i:i+seg_len] for i in range(X.shape[0]-seg_len)])\n",
    "target = np.array(y[seg_len:]).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.LSTM(input_size=5,hidden_size=10,num_layers=1).forward(torch.rand(3,5))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear(in_features=10,out_features=1).forward(torch.rand(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 10])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10,1,10)\n",
    "hidden = ((torch.zeros(2, x.size(0), 10)),\n",
    "                  (torch.zeros(2, x.size(0), 10)))\n",
    "\n",
    "x=nn.LSTM(input_size=10,hidden_size=10,num_layers=2,batch_first=True).forward(x,hidden)[0]\n",
    "# x = x.view(len(x),-1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(len(x),-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMmodel(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_num,out_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_size=hidden_num\n",
    "        self.in_size=input_dim\n",
    "        self.out_size = out_dim\n",
    "        self.lstm_layer = nn.LSTM(input_size=input_dim,hidden_size=hidden_num,batch_first=True)\n",
    "        self.hidden_layer = nn.Linear(in_features=hidden_num,out_features=out_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size = x.shape[0]\n",
    "        hidden = ((torch.zeros(1, self.hidden_size)),\n",
    "                  (torch.zeros(1, self.hidden_size)))\n",
    "        x,_ = self.lstm_layer(x,hidden)\n",
    "        out = self.hidden_layer(x)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=TensorDataset(\n",
    "    torch.Tensor(\n",
    "        serise[:int(len(serise)*0.7)]\n",
    "        )\n",
    "    ,torch.Tensor(\n",
    "        target[:int(len(serise)*0.7)]\n",
    "        )\n",
    "    )\n",
    "test_dataset=TensorDataset(\n",
    "    torch.Tensor(\n",
    "        serise[int(len(serise)*0.7):]\n",
    "        ),\n",
    "    torch.Tensor(\n",
    "        target[int(len(serise)*0.7):]\n",
    "        )\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_name, epochs=20, learning_rate=0.01):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    for epoch in tqdm(range(epochs), desc=f'Training {model_name}'):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "    return model, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LSTM: 100%|██████████| 20/20 [00:14<00:00,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "model, train_losses=train_model(LSTMmodel(5,10,1),\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\26921\\AppData\\Local\\Temp\\ipykernel_3020\\3858678591.py:15: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  df = pd.read_csv('data\\Alcohol_Sales.csv',index_col=0,parse_dates=True,encoding=\"GBK\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('data\\Alcohol_Sales.csv',index_col=0,parse_dates=True,encoding=\"GBK\")\n",
    "y = df['S4248SM144NCEN'].values.astype(float)\n",
    "test_size = 12\n",
    "train_set = y[:-test_size]  #323条数据\n",
    "test_set = y[-test_size:] #12条数据\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "train_norm = scaler.fit_transform(train_set.reshape(-1, 1)) \n",
    "\n",
    "train_norm = torch.FloatTensor(train_norm).view(-1) \n",
    "\n",
    "def input_data(seq,ws):  \n",
    "    out = []\n",
    "    L = len(seq)\n",
    "    for i in range(L-ws):\n",
    "        window = seq[i:i+ws]\n",
    "        label = seq[i+ws:i+ws+1]\n",
    "        out.append((window,label))  #将x和y以tensor格式放入到out列表当中，    \n",
    "    return out\n",
    "\n",
    "window_size = 12\n",
    "train_data = input_data(train_norm,window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "\n",
    "# 导入酒精销售数据\n",
    "df = pd.read_csv('data\\Alcohol_Sales.csv',index_col=0,parse_dates=True)\n",
    "len(df)\n",
    "\n",
    "df.head()  # 观察数据集，这是一个单变量时间序列\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.grid(True)\n",
    "plt.plot(df['S4248SM144NCEN'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "y = df['S4248SM144NCEN'].values.astype(float)\n",
    "\n",
    "# print(len(y))  #325条数据\n",
    "\n",
    "test_size = 12\n",
    "\n",
    "# 划分训练和测试集，最后12个值作为测试集\n",
    "train_set = y[:-test_size]  #323条数据\n",
    "test_set = y[-test_size:] #12条数据\n",
    "\n",
    "# print(train_set.shape) #(313,) 一位数组\n",
    "\n",
    "# 归一化至[-1,1]区间，为了获得更好的训练效果\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "#scaler.fit_transform输入必须是二维的，但是train_set却是一个一维，所有实验reshape(-1,1)\n",
    "train_norm = scaler.fit_transform(train_set.reshape(-1, 1)) #np.reshape(-1, 1) 列=1，行未知\n",
    "\n",
    "# print(train_norm.shape) #(313, 1) 这里将一维数据转化为二维\n",
    "\n",
    "# 转换成 tensor\n",
    "train_norm = torch.FloatTensor(train_norm).view(-1) \n",
    "print(train_norm.shape) #torch.Size([313])\n",
    "\n",
    "\n",
    "# 定义时间窗口，注意和前面的test size不是一个概念\n",
    "window_size = 12\n",
    "\n",
    "# 这个函数的目的是为了从原时间序列中抽取出训练样本，也就是用第一个值到第十二个值作为X输入，预测第十三个值作为y输出，这是一个用于训练的数据点，时间窗口向后滑动以此类推\n",
    "def input_data(seq,ws):  \n",
    "    out = []\n",
    "    L = len(seq)\n",
    "    for i in range(L-ws):\n",
    "        window = seq[i:i+ws]\n",
    "        label = seq[i+ws:i+ws+1]\n",
    "        out.append((window,label))  #将x和y以tensor格式放入到out列表当中，    \n",
    "    return out\n",
    "\n",
    "train_data = input_data(train_norm,window_size)\n",
    "len(train_data)  # 等于325（原始数据集长度）-12（测试集长度）-12（时间窗口）\n",
    "   \n",
    "class LSTMnetwork(nn.Module):\n",
    "    def __init__(self,input_size=1,hidden_size=100,output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # 定义LSTM层\n",
    "        self.lstm = nn.LSTM(input_size,hidden_size)\n",
    "        # 定义全连接层\n",
    "        self.linear = nn.Linear(hidden_size,output_size)\n",
    "        # 初始化h0，c0\n",
    "        self.hidden = (torch.zeros(1,1,self.hidden_size),\n",
    "                       torch.zeros(1,1,self.hidden_size))\n",
    "\n",
    "    def forward(self,seq):\n",
    "        # 前向传播的过程是输入->LSTM层->全连接层->输出\n",
    "\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html?highlight=lstm#torch.nn.LSTM\n",
    "        # 在观察查看LSTM输入的维度，LSTM的第一个输入input_size维度是(L, N, H_in), L是序列长度，N是batch size，H_in是输入尺寸，也就是变量个数\n",
    "        # LSTM的第二个输入是一个元组，包含了h0,c0两个元素，这两个元素的维度都是（D∗num_layers,N,H_out)，D=1表示单向网络，num_layers表示多少个LSTM层叠加，N是batch size，H_out表示隐层神经元个数\n",
    "        \n",
    "        '''\n",
    "        pytorch中LSTM输入为[time_step,batch,feature],这里窗口time_step=12,feature=1[1维数据]，batch我们这里设置为1\n",
    "        所以使用seq.view(len(seq),1,-1)将tensor[12]数据转化为tensor[12,1,1]\n",
    "        '''\n",
    "        lstm_out, self.hidden = self.lstm(seq.view(len(seq),1,-1), self.hidden)        \n",
    "        # print(lstm_out) #torch.Size([12, 1, 100]) [time_step,batch,hidden]        \n",
    "        # print(lstm_out.view(len(seq),-1))  #[12,100]\n",
    "        pred = self.linear(lstm_out.view(len(seq),-1)) \n",
    "        \n",
    "        # print(pred) #torch.Size([12, 1])\n",
    "        return pred[-1]  # 输出只用取最后一个值\n",
    "    \n",
    "torch.manual_seed(101)\n",
    "model = LSTMnetwork()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 100\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for seq, y_train in train_data:\n",
    "        \n",
    "        # 每次更新参数前都梯度归零和初始化\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = (torch.zeros(1,1,model.hidden_size),\n",
    "                        torch.zeros(1,1,model.hidden_size))\n",
    "\n",
    "        y_pred = model(seq)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch: {epoch+1:2} Loss: {loss.item():10.8f}')\n",
    "\n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds')\n",
    "\n",
    "future = 12\n",
    "\n",
    "# 选取序列最后12个值开始预测\n",
    "preds = train_norm[-window_size:].tolist()\n",
    "\n",
    "# 设置成eval模式\n",
    "model.eval()\n",
    "# 循环的每一步表示向时间序列向后滑动一格\n",
    "for i in range(future):\n",
    "    \n",
    "    seq = torch.FloatTensor(preds[-window_size:]) #第下一次循环的时候，seq总是能取到后12个数据，因此及时后面用pred.append()也还是每次用到最新的预测数据完成下一次的预测。\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.hidden = (torch.zeros(1,1,model.hidden_size),\n",
    "                        torch.zeros(1,1,model.hidden_size))\n",
    "        \"\"\"\n",
    "        item理解：取出张量具体位置的元素元素值，并且返回的是该位置元素值的高精度值，保持原元素类型不变；必须指定位置\n",
    "            即：原张量元素为整形，则返回整形，原张量元素为浮点型则返回浮点型，etc.\n",
    "        \"\"\"\n",
    "        # print(model(seq),model(seq).item())  #tensor([0.1027]), tensor([0.1026])\n",
    "        preds.append(model(seq).item())  #每循环一次，这里会将新的预测值添加到pred中，\n",
    "\n",
    "# 逆归一化还原真实值\n",
    "true_predictions = scaler.inverse_transform(np.array(preds[window_size:]).reshape(-1, 1))\n",
    "\n",
    "# 对比真实值和预测值\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.grid(True)\n",
    "plt.plot(df['S4248SM144NCEN'])\n",
    "x = np.arange('2018-02-01', '2019-02-01', dtype='datetime64[M]').astype('datetime64[D]')\n",
    "\n",
    "plt.plot(x,true_predictions)\n",
    "plt.show()\n",
    "\n",
    "# 放大看\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "plt.grid(True)\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "plt.plot(df['S4248SM144NCEN']['2017-01-01':])\n",
    "plt.plot(x,true_predictions)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 重新开始训练\n",
    "epochs = 100\n",
    "# 切回到训练模式\n",
    "model.train()\n",
    "y_norm = scaler.fit_transform(y.reshape(-1, 1))\n",
    "y_norm = torch.FloatTensor(y_norm).view(-1)\n",
    "all_data = input_data(y_norm,window_size)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for seq, y_train in all_data:  \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = (torch.zeros(1,1,model.hidden_size),\n",
    "                        torch.zeros(1,1,model.hidden_size))\n",
    "\n",
    "        y_pred = model(seq)\n",
    "\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch: {epoch+1:2} Loss: {loss.item():10.8f}')\n",
    "\n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds')\n",
    "\n",
    "# 重新预测\n",
    "window_size = 12\n",
    "future = 12\n",
    "L = len(y)\n",
    "\n",
    "preds = y_norm[-window_size:].tolist()\n",
    "\n",
    "model.eval()\n",
    "for i in range(future):  \n",
    "    seq = torch.FloatTensor(preds[-window_size:])\n",
    "    with torch.no_grad():\n",
    "\n",
    "        model.hidden = (torch.zeros(1,1,model.hidden_size),\n",
    "                        torch.zeros(1,1,model.hidden_size))  \n",
    "        preds.append(model(seq).item())\n",
    "\n",
    "\n",
    "true_predictions = scaler.inverse_transform(np.array(preds).reshape(-1, 1))\n",
    "\n",
    "\n",
    "x = np.arange('2019-02-01', '2020-02-01', dtype='datetime64[M]').astype('datetime64[D]')\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.grid(True)\n",
    "plt.plot(df['S4248SM144NCEN'])\n",
    "plt.plot(x,true_predictions[window_size:])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
