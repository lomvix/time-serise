{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, query_dim, context_dim):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.query_dim = query_dim\n",
    "        self.context_dim = context_dim\n",
    "\n",
    "        self.linear_q = nn.Linear(query_dim, query_dim)\n",
    "        self.linear_c = nn.Linear(context_dim, query_dim)\n",
    "\n",
    "    def forward(self, query, context):\n",
    "        # Query和Context的维度分别为 [batch_size, query_len, query_dim] 和 [batch_size, context_len, context_dim]\n",
    "        # 首先将Query和Context分别通过线性变换\n",
    "        query_proj = self.linear_q(query)  # [batch_size, query_len, query_dim]\n",
    "        context_proj = self.linear_c(context)  # [batch_size, context_len, query_dim]\n",
    "\n",
    "        # 计算注意力权重\n",
    "        attention_weights = torch.bmm(query_proj, context_proj.transpose(1, 2))  # [batch_size, query_len, context_len]\n",
    "        attention_weights = F.softmax(attention_weights, dim=-1)\n",
    "\n",
    "        # 对Context序列进行加权求和\n",
    "        attended_context = torch.bmm(attention_weights, context)  # [batch_size, query_len, context_dim]\n",
    "\n",
    "        return attended_context, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.randn(10, 3, 4)  \n",
    "tensor2 = torch.randn(4, 5)  \n",
    "torch.matmul(tensor1, tensor2).size()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m CrossAttention(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mforward(tensor1, tensor2)\n",
      "Cell \u001b[1;32mIn[8], line 21\u001b[0m, in \u001b[0;36mCrossAttention.forward\u001b[1;34m(self, query, context)\u001b[0m\n\u001b[0;32m     18\u001b[0m context_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_c(context)  \u001b[38;5;66;03m# [batch_size, context_len, query_dim]\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 计算注意力权重\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(query_proj, context_proj\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# [batch_size, query_len, context_len]\u001b[39;00m\n\u001b[0;32m     22\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(attention_weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 对Context序列进行加权求和\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "CrossAttention(4,5).forward(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scaling = self.head_dim ** -0.5\n",
    "        \n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # Shape: (batch_size, seq_len, embed_dim)\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Linear projections\n",
    "        q = self.q_linear(query)\n",
    "        k = self.k_linear(key)\n",
    "        v = self.v_linear(value)\n",
    "\n",
    "        # Reshape to (batch_size, num_heads, seq_len, head_dim)\n",
    "        q = q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Attention scores\n",
    "        attn_scores = torch.einsum('bhqd,bhkd->bhqk', q, k) * self.scaling\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        attn_output = torch.einsum('bhqk,bhvd->bhqd', attn_weights, v)\n",
    "\n",
    "        # Reshape back to (batch_size, seq_len, embed_dim)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)\n",
    "        \n",
    "        return self.out_linear(attn_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
