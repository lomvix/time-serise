{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "from torch.distributions import Normal,Uniform\n",
    "from torch.distributions.bernoulli import Bernoulli as torch_Bernoulli\n",
    "from torch import Tensor ,optim \n",
    "from typing import Optional, Union, Tuple\n",
    "from abc import abstractmethod, ABC \n",
    "import functools\n",
    "import math\n",
    "from torch.distributions.categorical import Categorical as torch_Categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_float(idx: np.ndarray, num_bins: int):\n",
    "    \"\"\"将离散化区间索引 k 转换为对应的区间中心值 k_c.\n",
    "    注意, 此处 k 的取值范围与论文中的不同, 论文中 k 的取值范围是 1~K, 而这里:\n",
    "    k_c = \\frac{2k+1}{K} - 1, where k \\in [0, K-1].\"\"\"\n",
    "    \n",
    "    flt_zero_one = (idx + 0.5) / num_bins\n",
    "    return (2.0 * flt_zero_one) - 1.0\n",
    "\n",
    "\n",
    "def float_to_idx(flt: np.ndarray, num_bins: int):\n",
    "    \"\"\"根据离散化值 k_c 计算出对应的区间索引 k, 是 float_to_idx() 的逆向操作.\"\"\"\n",
    "    \n",
    "    flt_zero_one = (flt / 2.0) + 0.5\n",
    "    return torch.clamp(torch.floor(flt_zero_one * num_bins), min=0, max=num_bins - 1).long()\n",
    "\n",
    "\n",
    "def quantize(flt, num_bins: int):\n",
    "    \"\"\"将浮点值量化以对应的离散化区间中点 k_c 表示, 因此看作是一个量化的过程.\"\"\"\n",
    "    return idx_to_float(float_to_idx(flt, num_bins), num_bins)\n",
    "\n",
    "\n",
    "def rgb_image_transform(x, num_bins=256):\n",
    "    \"\"\"将 RGB 图像进行离散化, 其中 x \\in [0,1]\"\"\"\n",
    "    return quantize((x * 2) - 1, num_bins).permute(1, 2, 0).contiguous()\n",
    "def sandwich(x: Tensor):\n",
    "    return x.reshape(x.size(0), -1, x.size(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONST_log_min = 1e-10\n",
    "\n",
    "\n",
    "def safe_log(data: Tensor):\n",
    "    return data.clamp(min=CONST_log_min).log()\n",
    "\n",
    "\n",
    "class CtsDistribution:\n",
    "    @abstractmethod\n",
    "    def log_prob(self, x):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def sample(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class DiscreteDistribution:\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def probs(self):\n",
    "        pass\n",
    "\n",
    "    @functools.cached_property\n",
    "    def log_probs(self):\n",
    "        return safe_log(self.probs)\n",
    "\n",
    "    @functools.cached_property\n",
    "    def mean(self):\n",
    "        pass\n",
    "\n",
    "    @functools.cached_property\n",
    "    def mode(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def log_prob(self, x):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def sample(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretizedDistribution(DiscreteDistribution):\n",
    "    def __init__(self, num_bins, device):\n",
    "        # 离散区间数量: K\n",
    "        self.num_bins = num_bins\n",
    "        # 原数据取值范围是[-1,1], 如今划分为 K 个区间, 因此每个区间宽度是 2/K.\n",
    "        self.bin_width = 2.0 / num_bins\n",
    "        self.half_bin_width = self.bin_width / 2.0\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    @functools.cached_property\n",
    "    def class_centres(self):\n",
    "        # 类别中心的取值范围: [-1 + 1/K, 1 - 1/K]\n",
    "        return torch.arange(self.half_bin_width - 1, 1, self.bin_width, device=self.device)\n",
    "\n",
    "    @functools.cached_property\n",
    "    def class_boundaries(self):\n",
    "        # 各类别之间的边界: [-1 + 2/K, 1 - 2/K], 共 K-1 个.\n",
    "        return torch.arange(self.bin_width - 1, 1 - self.half_bin_width, self.bin_width, device=self.device)\n",
    "\n",
    "    @functools.cached_property\n",
    "    def mean(self):\n",
    "        # 将各类别中心用它们各自所对应的概率加权求和: \\sum_{k=1}^K{p_k * k_c}\n",
    "        return (self.probs * self.class_centres).sum(-1)\n",
    "\n",
    "    @functools.cached_property\n",
    "    def mode(self):\n",
    "        \"\"\"概率分布的 mode, 代表众数, 即概率最高处所对应的样本.\"\"\"\n",
    "\n",
    "        # 因为 class_centres 是1维的, 所以这里需要将索引展平.\n",
    "        mode_idx = self.probs.argmax(-1).flatten()\n",
    "        return self.class_centres[mode_idx].reshape(self.probs.shape[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretizedCtsDistribution(DiscretizedDistribution):\n",
    "    \"\"\"将一个连续型分布离散化.\"\"\"\n",
    "    \n",
    "    def __init__(self, cts_dist, num_bins, device, batch_dims, clip=True, min_prob=1e-5):\n",
    "        super().__init__(num_bins, device)\n",
    "\n",
    "        # 原来的连续型分布, 要对其进行离散化处理.\n",
    "        self.cts_dist = cts_dist\n",
    "        # log(2/K)\n",
    "        self.log_bin_width = np.log(self.bin_width)\n",
    "        # B\n",
    "        self.batch_dims = batch_dims\n",
    "        \n",
    "        # 是否要对原来连续型分布的 CDF 做截断.\n",
    "        self.clip = clip\n",
    "        # 用作概率的极小值\n",
    "        self.min_prob = min_prob\n",
    "\n",
    "    @functools.cached_property\n",
    "    def probs(self):\n",
    "        \"\"\"计算数据位于各离散区间的概率.\"\"\"\n",
    "\n",
    "        # shape: [K-1] + [1] * B\n",
    "        bdry_cdfs = self.cts_dist.cdf(self.class_boundaries.reshape([-1] + ([1] * self.batch_dims)))\n",
    "        # shape: [1] + [1] * B\n",
    "        bdry_slice = bdry_cdfs[:1]\n",
    "        \n",
    "        if self.clip:\n",
    "            '''对原来连续型分布的 CDF 做截断: 小于第一个区间的左端概率置0、小于等于最后一个区间右端的概率置1.'''\n",
    "            \n",
    "            cdf_min = torch.zeros_like(bdry_slice)\n",
    "            cdf_max = torch.ones_like(bdry_slice)\n",
    "            # shape: [K+1] + [1] * B\n",
    "            bdry_cdfs = torch.cat([cdf_min, bdry_cdfs, cdf_max], 0)\n",
    "\n",
    "            # 利用 CDF(k_r) - CDF(k_l) 得到位于各区间的概率.\n",
    "            # shape: [1] * B + [K]\n",
    "            return (bdry_cdfs[1:] - bdry_cdfs[:-1]).moveaxis(0, -1)\n",
    "        else:\n",
    "            '''以条件概率的思想来计算数据位于各区间的概率，其中的条件就是数据位于 [-1,1] 取值范围内.\n",
    "            先计算原连续型分布在 1 和 -1 处的 CDF 值，将两者作差从而得到位于 [-1,1] 内的概率，以此作为条件对各区间的概率进行缩放.'''\n",
    "\n",
    "            # CDF(-1)\n",
    "            cdf_min = self.cts_dist.cdf(torch.zeros_like(bdry_slice) - 1)\n",
    "            # CDF(1)\n",
    "            cdf_max = self.cts_dist.cdf(torch.ones_like(bdry_slice))\n",
    "            # shape: [K+1] + [1] * B\n",
    "            bdry_cdfs = torch.cat([cdf_min, bdry_cdfs, cdf_max], 0)\n",
    "\n",
    "            # p_{-1 < x <= 1}\n",
    "            cdf_range = cdf_max - cdf_min\n",
    "            cdf_mask = cdf_range < self.min_prob\n",
    "            # 当 cdf_range 小于就以 1 代替, 避免作为分母时造成结果溢出.\n",
    "            cdf_range = torch.where(cdf_mask, (cdf_range * 0) + 1, cdf_range)\n",
    "\n",
    "            # shape: [K] + [1] * B\n",
    "            probs = (bdry_cdfs[1:] - bdry_cdfs[:-1]) / cdf_range\n",
    "            # 若整个 cdf_range 太小, 说明各区间的概率差异微不足道, 因此干脆将每个区间的概率都用 1/K 即均等的概率代替.\n",
    "            probs = torch.where(cdf_mask, (probs * 0) + (1 / self.num_bins), probs)\n",
    "\n",
    "            # shape: [1] * B + [K]\n",
    "            return probs.moveaxis(0, -1)\n",
    "\n",
    "    def prob(self, x):\n",
    "        # 区间索引 k \\in [0, K-1]\n",
    "        class_idx = float_to_idx(x, self.num_bins)\n",
    "        # 区间中心 k_c\n",
    "        centre = idx_to_float(class_idx, self.num_bins)\n",
    "        # CDF(k_l), 其中 k_l 代表区间左端点.\n",
    "        cdf_lo = self.cts_dist.cdf(centre - self.half_bin_width)\n",
    "        # CDF(k_r), 其中 k_r 代表区间右端点.\n",
    "        cdf_hi = self.cts_dist.cdf(centre + self.half_bin_width)\n",
    "        \n",
    "        if self.clip:\n",
    "            '''对原来连续型分布的 CDF 做截断, 使得:\n",
    "            CDF(k <= 0) = 0;\n",
    "            CDF(k >= K-1) = 1'''\n",
    "            \n",
    "            cdf_lo = torch.where(class_idx <= 0, torch.zeros_like(centre), cdf_lo)\n",
    "            cdf_hi = torch.where(class_idx >= (self.num_bins - 1), torch.ones_like(centre), cdf_hi)\n",
    "            \n",
    "            return cdf_hi - cdf_lo\n",
    "        else:\n",
    "            '''以条件概率的思想来计算数据位于某个离散区间内的概率，其中的条件就是数据位于 [-1,1] 取值范围内.\n",
    "            先计算原连续型分布在 1 和 -1 处的 CDF 值，将两者作差从而得到位于 [-1,1] 内的概率，以此作为条件对区间的概率进行缩放.'''\n",
    "            \n",
    "            cdf_min = self.cts_dist.cdf(torch.zeros_like(centre) - 1)\n",
    "            cdf_max = self.cts_dist.cdf(torch.ones_like(centre))\n",
    "            cdf_range = cdf_max - cdf_min\n",
    "            \n",
    "            # 若 cdf_range 太小，则设置 mask，并将其以1代替，即不对区间的概率进行缩放, 否则会使得计算出来的采样概率非常接近于1.\n",
    "            # 两个非常小的值相除, 由于它们都很小、非常接近，因此商接近于1.\n",
    "            cdf_mask = cdf_range < self.min_prob\n",
    "            cdf_range = torch.where(cdf_mask, (cdf_range * 0) + 1, cdf_range)\n",
    "            prob = (cdf_hi - cdf_lo) / cdf_range\n",
    "            \n",
    "            # 若整个 cdf_range 太小, 说明各区间的概率差异微不足道, 因此干脆将区间的概率都用 1/K 即均等的概率代替.\n",
    "            return torch.where(cdf_mask, (prob * 0) + (1 / self.num_bins), prob)\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        prob = self.prob(x)\n",
    "\n",
    "        return torch.where(\n",
    "            prob < self.min_prob,\n",
    "            # 将 x 以对应区间的中点 k_c 表示并计算出其在原来连续分布中的对数概率密度: log(p(k_c)).\n",
    "            # 这里加上 log(2/K) 相当于将 k_c 乘以 2/K 再取对数.\n",
    "            self.cts_dist.log_prob(quantize(x, self.num_bins)) + self.log_bin_width,\n",
    "            safe_log(prob),\n",
    "        )\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        if self.clip:\n",
    "            # 直接从原来的连续型分布中采样, 然后将其量化至对应的离散化区间.\n",
    "            # 此处, clip 的意思是:\n",
    "            # 若小于第一个区间，则以第一个区间中点表示；\n",
    "            # 同理，若大于最后一个区间，则以最后一个区间的中点表示.\n",
    "            return quantize(self.cts_dist.sample(sample_shape), self.num_bins)\n",
    "        else:\n",
    "            # 要求原来连续型分布的 CDF 存在反函数, 即可以根据概率值逆向求出对应的样本.\n",
    "            assert hasattr(self.cts_dist, \"icdf\")\n",
    "            \n",
    "            # 数据的取值范围是 [-1,1], 先根据原来的连续型分布计算出 CDF(-1) 和 CDF(1),\n",
    "            # 然后利用 CDF 的反函数仅在这个 range 内考虑采样.\n",
    "            cdf_min = self.cts_dist.cdf(torch.zeros_like(self.cts_dist.mean) - 1)\n",
    "            cdf_max = self.cts_dist.cdf(torch.ones_like(cdf_min))\n",
    "\n",
    "            # 由于 CDF 是服从均匀分布的, 因此从均匀分布中采样出 CDF 值并利用反函数求出对应样本就等价于从目标分布中采样.\n",
    "            u = Uniform(cdf_min, cdf_max, validate_args=False).sample(sample_shape)\n",
    "            cts_samp = self.cts_dist.icdf(u)\n",
    "\n",
    "            # 最后将样本量化至对应的离散化区间.\n",
    "            # 注意, 与前面 clip 的方式不同, 此处在量化前样本已经处于有效的离散化区间内了, 因为采样区间是在[-1,1]内考虑的.\n",
    "            return quantize(cts_samp, self.num_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONST_exp_range = 10\n",
    "\n",
    "\n",
    "def safe_exp(data: Tensor):\n",
    "    return data.clamp(min=-CONST_exp_range, max=CONST_exp_range).exp()\n",
    "\n",
    "\n",
    "class DiscretizedNormal(DiscretizedCtsDistribution):\n",
    "    def __init__(self, params, num_bins, clip=False, min_std_dev=1e-3, max_std_dev=10, min_prob=1e-5, log_dev=True):\n",
    "        assert params.size(-1) == 2\n",
    "        \n",
    "        if min_std_dev < 0:\n",
    "            min_std_dev = 1.0 / (num_bins * 5)\n",
    "            \n",
    "        mean, std_dev = params.split(1, -1)[:2]\n",
    "        if log_dev:\n",
    "            # 若传入的是对数标准差, 那么此处就需要取自然指数进行还原.\n",
    "            std_dev = safe_exp(std_dev)\n",
    "        std_dev = std_dev.clamp(min=min_std_dev, max=max_std_dev)\n",
    "        \n",
    "        super().__init__(\n",
    "            cts_dist=Normal(mean.squeeze(-1), std_dev.squeeze(-1), validate_args=False),\n",
    "            num_bins=num_bins,\n",
    "            device=params.device,\n",
    "            # 注意所谓的 batch dims 并非指数据的 batch size,\n",
    "            # 而是除离散化区间数量以外与分布本身关系不大的其它维度.\n",
    "            batch_dims=params.ndim - 1,\n",
    "            clip=clip,\n",
    "            min_prob=min_prob,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeltaDistribution(CtsDistribution):\n",
    "    def __init__(self, mean, clip_range=1.0):\n",
    "        if clip_range > 0:\n",
    "            mean = mean.clip(min=-clip_range, max=clip_range)\n",
    "        self.mean = mean\n",
    "\n",
    "    def mode(self):\n",
    "        return self.mean\n",
    "\n",
    "    def mean(self):\n",
    "        return self.mean\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        return self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bernoulli(DiscreteDistribution):\n",
    "    def __init__(self, logits):\n",
    "        self.bernoulli = torch_Bernoulli(logits=logits, validate_args=False)\n",
    "\n",
    "    @functools.cached_property\n",
    "    def probs(self):\n",
    "        p = self.bernoulli.probs.unsqueeze(-1)\n",
    "        return torch.cat([1 - p, p], -1)\n",
    "\n",
    "    @functools.cached_property\n",
    "    def mode(self):\n",
    "        return self.bernoulli.mode\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        return self.bernoulli.log_prob(x.float())\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        return self.bernoulli.sample(sample_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Categorical(DiscreteDistribution):\n",
    "    def __init__(self, logits):\n",
    "        self.categorical = torch_Categorical(logits=logits, validate_args=False)\n",
    "        self.n_classes = logits.size(-1)\n",
    "\n",
    "    @functools.cached_property\n",
    "    def probs(self):\n",
    "        return self.categorical.probs\n",
    "\n",
    "    @functools.cached_property\n",
    "    def mode(self):\n",
    "        return self.categorical.mode\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        return self.categorical.log_prob(x)\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        return self.categorical.sample(sample_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_pred_params_to_data_pred_params(\n",
    "    noise_pred_params: torch.Tensor, input_mean: torch.Tensor,\n",
    "    t: torch.Tensor, min_variance: float, min_t=1e-6\n",
    "):\n",
    "    \"\"\"Convert output parameters that predict the noise added to data, to parameters that predict the data.\n",
    "    将模型预测的噪声分布的参数转换为数据分布的参数.\"\"\"\n",
    "\n",
    "    # (B,L,D)\n",
    "    data_shape = list(noise_pred_params.shape)[:-1]\n",
    "    # (B,L*D,NP), NP: num parameters per data\n",
    "    noise_pred_params = sandwich(noise_pred_params)\n",
    "    # (B,L*D)\n",
    "    input_mean = input_mean.flatten(start_dim=1)\n",
    "    \n",
    "    if torch.is_tensor(t):\n",
    "        t = t.flatten(start_dim=1)\n",
    "    else:\n",
    "        t = (input_mean * 0) + t\n",
    "        \n",
    "    # (B,L*D,1)\n",
    "    alpha_mask = (t < min_t).unsqueeze(-1)\n",
    "    \n",
    "    # \\sigma_1^{2t}\n",
    "    posterior_var = torch.pow(min_variance, t.clamp(min=min_t))\n",
    "    # \\gamma(t) = 1 - \\sigma_1^{2t}\n",
    "    gamma = 1 - posterior_var\n",
    "\n",
    "    # \\frac{\\mu}{\\gamma(t)}\n",
    "    A = (input_mean / gamma).unsqueeze(-1)\n",
    "    # \\sqrt{\\frac{1-\\gamma(t)}{\\gamma(t)}}\n",
    "    B = (posterior_var / gamma).sqrt().unsqueeze(-1)\n",
    "    \n",
    "    data_pred_params = []\n",
    "    \n",
    "    # 对应建模连续数据的场景: 模型预测的是噪声向量.\n",
    "    if noise_pred_params.size(-1) == 1:\n",
    "        noise_pred_mean = noise_pred_params\n",
    "    # 对应建模离散化数据的场景: 模型预测的是噪声分布的均值与对数标准差. \n",
    "    elif noise_pred_params.size(-1) == 2:\n",
    "        noise_pred_mean, noise_pred_log_dev = noise_pred_params.chunk(2, -1)\n",
    "    else:\n",
    "        assert noise_pred_params.size(-1) % 3 == 0\n",
    "        mix_wt_logits, noise_pred_mean, noise_pred_log_dev = noise_pred_params.chunk(3, -1)\n",
    "        data_pred_params.append(mix_wt_logits)\n",
    "\n",
    "    # 连续数据: x = \\frac{\\mu}{\\gamma(t)} - \\sqrt{\\frac{1-\\gamma(t)}{\\gamma(t)}} \\epsilon\n",
    "    # 离散化数据: \\mu_{x} = \\frac{\\mu}{\\gamma(t)} - \\sqrt{\\frac{1-\\gamma(t)}{\\gamma(t)}} \\mu_{\\epsilon}\n",
    "    data_pred_mean = A - (B * noise_pred_mean)\n",
    "    # 时间变量的值过小则被认为是起始时刻, 等同于先验形式, 即标准高斯分布, 于是将预测的均值置0\n",
    "    data_pred_mean = torch.where(alpha_mask, 0 * data_pred_mean, data_pred_mean)\n",
    "    data_pred_params.append(data_pred_mean)\n",
    "    \n",
    "    if noise_pred_params.size(-1) >= 2:\n",
    "        # 将对数标准差取自然指数复原: exp(ln(\\sigma_{\\epsilon})) -> \\sigma_{\\epsilon}\n",
    "        noise_pred_dev = safe_exp(noise_pred_log_dev)\n",
    "        # 将噪声分布的标准差转换为目标数据分布的标准差: \\sqrt{\\frac{1-\\gamma(t)}{\\gamma(t)}} exp(ln(\\sigma_{\\epsilon})) -> \\mu_x\n",
    "        data_pred_dev = B * noise_pred_dev\n",
    "        # 时间变量的值过小则被认为是起始时刻, 等同于先验形式, 即标准高斯分布, 于是将预测的标准差置1\n",
    "        data_pred_dev = torch.where(alpha_mask, 1 + (0 * data_pred_dev), data_pred_dev)\n",
    "        data_pred_params.append(data_pred_dev)\n",
    "\n",
    "    # (B,L*D,NP)\n",
    "    data_pred_params = torch.cat(data_pred_params, -1)\n",
    "    # (B,L,D,NP)\n",
    "    data_pred_params = data_pred_params.reshape(data_shape + [-1])\n",
    "    \n",
    "    return data_pred_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CtsDistributionFactory:\n",
    "    @abstractmethod\n",
    "    def get_dist(self, params: torch.Tensor, input_params=None, t=None) -> CtsDistribution:\n",
    "        \"\"\"Note: input_params and t are not used but kept here to be consistency with DiscreteDistributionFactory.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class DiscreteDistributionFactory:\n",
    "    @abstractmethod\n",
    "    def get_dist(self, params: torch.Tensor, input_params=None, t=None) -> DiscreteDistribution:\n",
    "        \"\"\"Note: input_params and t are only required by PredDistToDataDistFactory.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class DiscretizedNormalFactory(DiscreteDistributionFactory):\n",
    "    def __init__(self, num_bins, clip=True, min_std_dev=1e-3, max_std_dev=10, min_prob=1e-5, log_dev=True):\n",
    "        self.num_bins = num_bins\n",
    "        self.clip = clip\n",
    "        self.min_std_dev = min_std_dev\n",
    "        self.max_std_dev = max_std_dev\n",
    "        self.min_prob = min_prob\n",
    "        self.log_dev = log_dev\n",
    "\n",
    "    def get_dist(self, params, input_params=None, t=None):\n",
    "        return DiscretizedNormal(\n",
    "            params,\n",
    "            num_bins=self.num_bins,\n",
    "            clip=self.clip,\n",
    "            min_std_dev=self.min_std_dev,\n",
    "            max_std_dev=self.max_std_dev,\n",
    "            min_prob=self.min_prob,\n",
    "            log_dev=self.log_dev,\n",
    "        )\n",
    "\n",
    "\n",
    "class DeltaFactory(CtsDistributionFactory):\n",
    "    def __init__(self, clip_range=1.0):\n",
    "        self.clip_range = clip_range\n",
    "\n",
    "    def get_dist(self, params, input_params=None, t=None):\n",
    "        return DeltaDistribution(params.squeeze(-1), self.clip_range)\n",
    "\n",
    "\n",
    "class BernoulliFactory(DiscreteDistributionFactory):\n",
    "    def get_dist(self, params, input_params=None, t=None):\n",
    "        return Bernoulli(logits=params.squeeze(-1))\n",
    "\n",
    "\n",
    "class CategoricalFactory(DiscreteDistributionFactory):\n",
    "    def get_dist(self, params, input_params=None, t=None):\n",
    "        return Categorical(logits=params)\n",
    "    \n",
    "class PredDistToDataDistFactory(DiscreteDistributionFactory):\n",
    "    def __init__(self, data_dist_factory, min_variance, min_t=1e-6):\n",
    "        self.data_dist_factory = data_dist_factory\n",
    "        # 之所以设为 False 是因为在以下 noise_pred_params_to_data_pred_params() 方法中会将对数标准差使用自然指数进行转换,\n",
    "        # 而无需原数据分布的工厂自行转换.\n",
    "        self.data_dist_factory.log_dev = False\n",
    "        self.min_variance = min_variance\n",
    "        self.min_t = min_t\n",
    "\n",
    "    def get_dist(self, params, input_params, t):\n",
    "        data_pred_params = noise_pred_params_to_data_pred_params(params, input_params[0], t, self.min_variance, self.min_t)\n",
    "        return self.data_dist_factory.get_dist(data_pred_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BayesianFlow class -CtsBayesianFlow and DiscreteBayesianFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianFlow(nn.Module, ABC):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_prior_input_params(self, data_shape: tuple, device: torch.device) -> tuple[Tensor, ...]:\n",
    "        \"\"\"Returns the initial input params (for a batch) at t=0. Used during sampling.\n",
    "        For discrete data, the tuple has length 1 and contains the initial class probabilities.\n",
    "        For continuous data, the tuple has length 2 and contains the mean and precision.\n",
    "        \n",
    "        返回起始时刻的先验参数, 作为模型的输入, 方法用于采样过程的开端.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def params_to_net_inputs(self, params: tuple[Tensor, ...]) -> Tensor:\n",
    "        \"\"\"Utility method to convert input distribution params to network inputs if needed.\n",
    "        \n",
    "        如果有必要的话, 将输入分布的参数转换为适合模型输入的形式.\n",
    "        比如在建模离散化数据时, 输入分布的参数代表概率, 取值范围在[0,1], 于是在输入模型前会将其 scale 至[-1,1],\n",
    "        从而与其他类型的数据场景兼容, 并且避免让模型永远只接收非负值.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_alpha(self, i: Union[int, Tensor], n_steps: int) -> float:\n",
    "        \"\"\"Returns the alpha at step i of total n_steps according to the flow schedule. Used:\n",
    "        a) during sampling, when i and alpha are the same for all samples in the batch.\n",
    "        b) during discrete time loss computation, when i and alpha are different for samples in the batch.\n",
    "        \n",
    "        计算某个离散时间步所对应的精度: /alpha_i = /beta(t_i) - /beta(t_{i-1}), 用于采样过程或离散时间的损失函数. \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_sender_dist(self, x: Tensor, alpha: Union[float, Tensor], shape=torch.Size([])) -> D.Distribution:\n",
    "        \"\"\"Returns the sender distribution with accuracy alpha obtained by adding appropriate noise to the data x. Used:\n",
    "        a) during sampling (same alpha for whole batch) to sample from the output distribution produced by the net.\n",
    "        b) during discrete time loss computation when alpha are different for samples in the batch.\n",
    "        \n",
    "        返回指定精度 \\alpha 下的输入分布. \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update_input_params(self, input_params: tuple[Tensor, ...], y: Tensor, alpha: float) -> tuple[Tensor, ...]:\n",
    "        \"\"\"Updates the distribution parameters using Bayes' theorem in light of noisy sample y.\n",
    "        Used during sampling when alpha is the same for the whole batch.\n",
    "        \n",
    "        根据贝叶斯定理利用观测样本 y 计算后验, 从而更新先验. \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, data: Tensor, t: Tensor) -> tuple[Tensor, ...]:\n",
    "        \"\"\"Returns a sample from the Bayesian Flow distribution over input parameters at time t conditioned on data.\n",
    "        Used during training when t (and thus accuracies) are different for different samples in the batch.\n",
    "        For discrete data, the returned tuple has length 1 and contains the class probabilities.\n",
    "        For continuous data, the returned tuple has length 2 and contains the mean and precision.\n",
    "        \n",
    "        从贝叶斯流分布中采样得到后验, 代表对输入分布参数的更新. \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CtsBayesianFlow(BayesianFlow):\n",
    "    \"\"\"建模连续/离散化数据的贝叶斯流.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        min_variance: float = 1e-6,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.min_variance = min_variance\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, data: Tensor, t: Tensor) -> tuple[Tensor, None]:\n",
    "        \"\"\"返回贝叶斯流分布的采样结果, 即经过后验更新的输入分布的均值向量: \\mu.\"\"\"\n",
    "        \n",
    "        # \\omega_1^{2t}\n",
    "        post_var = torch.pow(self.min_variance, t)\n",
    "        # \\gamma(t)\n",
    "        alpha_t = 1 - post_var\n",
    "        # \\gamma(t)(1-\\gamma(t))\n",
    "        mean_var = alpha_t * post_var\n",
    "        \n",
    "        # 贝叶斯流分布的均值: \\gamma(t)x\n",
    "        mean_mean = alpha_t * data\n",
    "        # 贝叶斯流分布的标准差: \\sqrt{\\gamma(t)(1-\\gamma(t))}\n",
    "        mean_std_dev = mean_var.sqrt()\n",
    "        \n",
    "        # 标准高斯噪声\n",
    "        noise = torch.randn(mean_mean.shape, device=mean_mean.device)\n",
    "        # 利用重参数化技术构造贝叶斯流分布的样本\n",
    "        mean = mean_mean + (mean_std_dev * noise)\n",
    "        \n",
    "        # We don't need to compute the variance because it is not needed by the network, so set it to None\n",
    "        input_params = (mean, None)\n",
    "        \n",
    "        return input_params\n",
    "    def params_to_net_inputs(self, params: tuple[Tensor]) -> Tensor:\n",
    "        # 仅取输入分布的均值向量作为 BFN 的输入\n",
    "        # Only the mean is used by the network\n",
    "        return params[0]\n",
    "\n",
    "    def get_prior_input_params(self, data_shape: tuple, device: torch.device) -> tuple[Tensor, float]:\n",
    "        # 起始时刻的先验是标准高斯分布, 均值为0, 方差为1(协方差矩阵是对角元均为1的对角阵)\n",
    "        return torch.zeros(*data_shape, device=device), 1.0\n",
    "\n",
    "    def get_alpha(self, i: Union[int, Tensor], n_steps: int) -> Union[float, Tensor]:\n",
    "        # 根据 \\beta(t_i) - \\beta(t_{i-1}) 计算, 其中 t_i = \\frac{i}{n}.\n",
    "        sigma_1 = math.sqrt(self.min_variance)\n",
    "        return (sigma_1 ** (-2 * i / n_steps)) * (1 - sigma_1 ** (2 / n_steps))\n",
    "\n",
    "    def get_sender_dist(self, x: Tensor, alpha: Union[float, Tensor], shape=torch.Size([])) -> D.Distribution:\n",
    "        # 返回输入分布, 精度 \\alpha 是方差的倒数.\n",
    "        dist = D.Normal(x, 1.0 / alpha**0.5)\n",
    "        return dist\n",
    "\n",
    "    def update_input_params(self, input_params: tuple[Tensor, float], y: Tensor, alpha: float) -> tuple[Tensor, float]:\n",
    "        \"\"\"贝叶斯更新函数, 对输入分布的参数进行后验更新.\"\"\"\n",
    "        \n",
    "        input_mean, input_precision = input_params\n",
    "        # \\rho_i = \\rho_{i-1} + \\alpha\n",
    "        new_precision = input_precision + alpha\n",
    "        # 根据贝叶斯定理计算: \\mu_i = \\frac{ \\rho_{i-1} \\mu_{i-1} + \\alpha y }{\\rho_i}\n",
    "        new_mean = ((input_precision * input_mean) + (alpha * y)) / new_precision\n",
    "        \n",
    "        return new_mean, new_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteBayesianFlow(BayesianFlow):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes: int,\n",
    "        min_sqrt_beta: float = 1e-10,\n",
    "        discretize: bool = False,\n",
    "        epsilon: float = 1e-6,\n",
    "        max_sqrt_beta: float = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # K\n",
    "        self.n_classes = n_classes\n",
    "        # 一个极小值, 用于将传入贝叶斯流分布的时间变量最大值限制至 1-epsilon.\n",
    "        # 因为贝叶斯流分布是用于最终时刻前的, 所以需要 t < 1.\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # 是否进行离散化操作\n",
    "        self.discretize = discretize\n",
    "        \n",
    "        # \\sqrt{\\beta} 的下限\n",
    "        self.min_sqrt_beta = min_sqrt_beta\n",
    "        # \\sqrt{\\beta(1)}\n",
    "        self.max_sqrt_beta = max_sqrt_beta\n",
    "        \n",
    "        # 均匀分布的期望熵: H = - \\sum_{i=1}^K{p(x_i)ln(p(x_i))}, p(x_i)=\\frac{1}{K}\n",
    "        self.uniform_entropy = math.log(self.n_classes)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, data: Tensor, t: Tensor) -> tuple[Tensor]:\n",
    "        \"\"\"根据贝叶斯流分布完成后验更新.\"\"\"\n",
    "        \n",
    "        if self.discretize:\n",
    "            # 若要进行离散化操作, 则将数据以对应的离散化区间索引表示.\n",
    "            data = float_to_idx(data, self.n_classes)\n",
    "        \n",
    "        # \\sqrt{\\beta(t)}\n",
    "        sqrt_beta = self.t_to_sqrt_beta(t.clamp(max=1 - self.epsilon))\n",
    "        lo_beta = sqrt_beta < self.min_sqrt_beta\n",
    "        sqrt_beta = sqrt_beta.clamp(min=self.min_sqrt_beta)\n",
    "        # \\beta(t)\n",
    "        beta = sqrt_beta.square().unsqueeze(-1)\n",
    "        \n",
    "        # 从精度参数为 \\beta(t) 的发送者分布中采样观测样本以作为贝叶斯流分布的 logits.\n",
    "        logits = self.count_sample(data, beta)\n",
    "        probs = F.softmax(logits, -1)\n",
    "        # 将精度太小的部分所对应的后验以均匀先验 \\frac{1}{K} 代替.\n",
    "        # 这是因为精度太小, 那么对应的观测样本也\"不靠谱\"——所包含真实数据的信息太少,\n",
    "        # 将其作为 logits 就不靠谱, 即以此为根据而实现的后验更新意义不大.\n",
    "        probs = torch.where(lo_beta.unsqueeze(-1), torch.ones_like(probs) / self.n_classes, probs)\n",
    "        if self.n_classes == 2:\n",
    "            # 如果是二分类则只取其中一类的概率即可.\n",
    "            probs = probs[..., :1]\n",
    "            probs = probs.reshape_as(data)\n",
    "            \n",
    "        input_params = (probs,)\n",
    "        \n",
    "        return input_params\n",
    "\n",
    "    def t_to_sqrt_beta(self, t):\n",
    "        \"\"\"计算当前时刻的 accuracy schedule: \\beta(t) 的开根:\n",
    "           sqrt{\\beta(t)} = t \\sqrt{\\beta(1)}.\"\"\"\n",
    "        \n",
    "        return t * self.max_sqrt_beta\n",
    "\n",
    "    def count_dist(self, x, beta=None) -> D.Distribution:\n",
    "        \"\"\"贝叶斯流分布中的期望部分所对应的发送者分布.\"\"\"\n",
    "\n",
    "        # Ke_x - 1\n",
    "        mean = (self.n_classes * F.one_hot(x.long(), self.n_classes)) - 1\n",
    "        # \\sqrt{K}\n",
    "        std_dev = math.sqrt(self.n_classes)\n",
    "        \n",
    "        if beta is not None:\n",
    "            # \\beta(t)(Ke_x - 1)\n",
    "            mean = mean * beta\n",
    "            # \\sqrt{\\beta(t)K}\n",
    "            std_dev = std_dev * beta.sqrt()\n",
    "            \n",
    "        return D.Normal(mean, std_dev, validate_args=False)\n",
    "\n",
    "    def count_sample(self, x, beta):\n",
    "        \"\"\"利用重参数化采样技术(rsample())采样出观测样本作为贝叶斯流分布的 logits 源(下一步将其输入 softmax 以实现后验更新).\"\"\"\n",
    "        return self.count_dist(x, beta).rsample()\n",
    "\n",
    "    def float_to_idx(data: Tensor, n_classes: int) -> Tensor:\n",
    "        \"\"\"Convert continuous data to discrete indices.\"\"\"\n",
    "        # Assuming data is normalized between 0 and 1\n",
    "        return (data * n_classes).long().clamp(0, n_classes - 1)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_prior_input_params(self, data_shape: tuple, device: torch.device) -> tuple[Tensor]:\n",
    "        \"\"\"初始先验: 各类别概率相等的均匀分布 U{1, K}.\"\"\"\n",
    "        \n",
    "        # 注意返回的是元组, 这是为了与连续/离散化数据的场景保持一致性.\n",
    "        return (torch.ones(*data_shape, self.n_classes, device=device) / self.n_classes,)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def params_to_net_inputs(self, params: tuple[Tensor]) -> Tensor:\n",
    "        params = params[0]\n",
    "        if self.n_classes == 2:\n",
    "            # 作者使用的 MNIST 数据集是经过二值化处理的, 因此这部分针对 MNIST 操作,\n",
    "            # 将模型输入的范围缩放至 [-1,1]\n",
    "            params = params * 2 - 1  # We scale-shift here for MNIST instead of in the network like for text\n",
    "            # 因为总共只有两个类别, 所以取其中一类所对应的概率即可.\n",
    "            params = params[..., :1]\n",
    "            \n",
    "        return params\n",
    "\n",
    "    def get_alpha(self, i: Union[int, Tensor], n_steps: int) -> Union[float, Tensor]:\n",
    "        # 计算离散时间步所对应的精度: \\alpha_i = \\beta(1) \\frac{2i-1}{n^2}\n",
    "        return ((self.max_sqrt_beta / n_steps) ** 2) * (2 * i - 1)\n",
    "\n",
    "    def get_sender_dist(self, x: Tensor, alpha: Union[float, Tensor], shape=torch.Size([])) -> D.Distribution:\n",
    "        e_x = F.one_hot(x.long(), self.n_classes)\n",
    "        alpha = alpha.unsqueeze(-1) if isinstance(alpha, Tensor) else alpha\n",
    "        dist = D.Normal(alpha * ((self.n_classes * e_x) - 1), (self.n_classes * alpha) ** 0.5)\n",
    "        \n",
    "        return dist\n",
    "\n",
    "    def update_input_params(self, input_params: tuple[Tensor], y: Tensor, alpha: float) -> tuple[Tensor]:\n",
    "        \"\"\"贝叶斯更新函数: 利用贝叶斯定理计算后验.\"\"\"\n",
    "        \n",
    "        new_input_params = input_params[0] * y.exp()\n",
    "        new_input_params /= new_input_params.sum(-1, keepdims=True)\n",
    "        \n",
    "        # 注意返回的是元组\n",
    "        return (new_input_params,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss class -cts and discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module, ABC):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def cts_time_loss(self, data: Tensor, output_params: Tensor, input_params: Tensor, t: Tensor) -> Tensor:\n",
    "        \"\"\"Returns the continuous time KL loss (and any other losses) at time t (between 0 and 1).\n",
    "        The input params are only used when the network is parameterized to predict the noise for continuous data.\n",
    "        \n",
    "        连续时间的损失函数. \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def discrete_time_loss(\n",
    "        self, data: Tensor,\n",
    "        output_params: Tensor, input_params: Tensor,\n",
    "        t: Tensor, n_steps: int, n_samples: int = 20\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Returns the discrete time KL loss for n_steps total of communication at time t (between 0 and 1) using\n",
    "        n_samples for Monte Carlo estimation of the discrete loss.\n",
    "        The input params are only used when the network is parameterized to predict the noise for continuous data.\n",
    "        \n",
    "        离散时间的损失函数, 当所需计算的 KL 散度没有解析形式时, 使用蒙特卡洛方法来近似估计. \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def reconstruction_loss(self, data: Tensor, output_params: Tensor, input_params: Tensor) -> Tensor:\n",
    "        \"\"\"Returns the reconstruction loss, i.e. the final cost of transmitting clean data.\n",
    "        The input params are only used when the network is parameterized to predict the noise for continuous data.\n",
    "        \n",
    "        重构损失, 不参与训练. \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CtsBayesianFlowLoss(Loss):\n",
    "    \"\"\"建模连续/离散化数据场景时所用的损失函数, 包括：\n",
    "    -离散时间损失函数;\n",
    "    -连续时间损失函数;\n",
    "    -重构损失\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        bayesian_flow: CtsBayesianFlow,\n",
    "        distribution_factory: Union[CtsDistributionFactory, DiscreteDistributionFactory],\n",
    "        min_loss_variance: float = -1,\n",
    "        noise_pred: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bayesian_flow = bayesian_flow\n",
    "        # 返回输出分布的factory对象\n",
    "        self.distribution_factory = distribution_factory\n",
    "        # \\sigma_1^{2} 的下限, 以防用作分母时溢出.\n",
    "        self.min_loss_variance = min_loss_variance\n",
    "        # -ln(\\sigma_1)\n",
    "        self.C = -0.5 * math.log(bayesian_flow.min_variance)\n",
    "        \n",
    "        # 是否预测噪声(亦或是直接预测数据)\n",
    "        self.noise_pred = noise_pred\n",
    "        if self.noise_pred:\n",
    "            self.distribution_factory.log_dev = False\n",
    "            # 在预测噪声的情况下, 将预测的噪声(或噪声分布相关的参数)转换为对应数据分布(输出分布)的参数.\n",
    "            self.distribution_factory = PredDistToDataDistFactory(\n",
    "                self.distribution_factory, self.bayesian_flow.min_variance\n",
    "            )\n",
    "\n",
    "    def cts_time_loss(self, data: Tensor, output_params: Tensor, input_params: Tensor, t) -> Tensor:\n",
    "        # 模型输出\n",
    "        # reshape 成3维:(B, -1, D)\n",
    "        output_params = sandwich(output_params)\n",
    "\n",
    "        \n",
    "        t = t.flatten(start_dim=1).float()\n",
    "        flat_target = data.flatten(start_dim=1)\n",
    "        \n",
    "        # \\sigma_1^{2t}\n",
    "        posterior_var = torch.pow(self.bayesian_flow.min_variance, t)\n",
    "        if self.min_loss_variance > 0:\n",
    "            # 做最小值截断, 以防其作分母时防止溢出\n",
    "            posterior_var = posterior_var.clamp(min=self.min_loss_variance)\n",
    "        \n",
    "        # 输出分布\n",
    "        pred_dist = self.distribution_factory.get_dist(output_params, input_params, t)\n",
    "        # 输出分布的均值 E[P(\\theta, t)]\n",
    "        pred_mean = pred_dist.mean\n",
    "        \n",
    "        mse_loss = (pred_mean - flat_target).square()\n",
    "        # 连续时间的损失函数计算公式: -ln(\\sigma_1) \\sigma_1{-2t} || x - E[P(\\theta, t)] ||^2\n",
    "        loss = self.C * mse_loss / posterior_var\n",
    "        \n",
    "        return loss\n",
    "    def discrete_time_loss(\n",
    "        self, data: Tensor,\n",
    "        output_params: Tensor, input_params: Tensor,\n",
    "        t: Tensor, n_steps: int, n_samples=10\n",
    "    ) -> Tensor:\n",
    "        # (B,-1,D)\n",
    "        output_params = sandwich(output_params)\n",
    "        t = t.flatten(start_dim=1).float()\n",
    "        \n",
    "        output_dist = self.distribution_factory.get_dist(output_params, input_params, t)\n",
    "\n",
    "        # 离散化数据的场景\n",
    "        if hasattr(output_dist, \"probs\"):  # output distribution is discretized normal\n",
    "            t = t.flatten(start_dim=1)\n",
    "            i = t * n_steps + 1  # since t = (i - 1) / n\n",
    "            \n",
    "            alpha = self.bayesian_flow.get_alpha(i, n_steps)\n",
    "            \n",
    "            flat_target = data.flatten(start_dim=1)\n",
    "            # 发送者分布\n",
    "            sender_dist = self.bayesian_flow.get_sender_dist(flat_target, alpha)\n",
    "            # 因为使用蒙特卡洛方法来估计发送者分布与接收者分布之间的 KL 散度，所以要从发送者分布中采样观测样本 y,\n",
    "            # 采样的样本数默认为10.\n",
    "            y = sender_dist.sample(torch.Size([n_samples]))\n",
    "            \n",
    "            # 模型输出的分配到各离散化区间的概率值. \n",
    "            #(B,D,K)\n",
    "            receiver_mix_wts = sandwich(output_dist.probs)\n",
    "            # 输出分布是类别分布, 在每个离散化区间都分配一定概率.\n",
    "            receiver_mix_dist= D.Categorical(probs=receiver_mix_wts, validate_args=False)\n",
    "            # 以各离散化区间的中心为均值构造多个一维高斯分布，其中每个都与发送者分布的形式一致(噪声强度相等, 即方差一致).\\\n",
    "            receiver_components = D.Normal(\n",
    "                output_dist.class_centres, (1.0 / alpha.sqrt()).unsqueeze(-1), validate_args=False\n",
    "            )\n",
    "            # 接收者分布, 在数据的每个维度上都是混合高斯分布.\n",
    "            receiver_dist = D.MixtureSameFamily(receiver_mix_dist, receiver_components, validate_args=False)\n",
    "            \n",
    "            # (B,1)\n",
    "            loss = (\n",
    "                (sender_dist.log_prob(y) - receiver_dist.log_prob(y))  # 发送者分布和接收者分布的概率密度对数差\n",
    "                .mean(0)  # 在蒙特卡洛采样的样本数上做平均\n",
    "                .flatten(start_dim=1)\n",
    "                .mean(1, keepdims=True)\n",
    "            )\n",
    "        # 连续数据的场景\n",
    "        else:  # output distribution is normal\n",
    "            pred_mean = output_dist.mean\n",
    "            flat_target = data.flatten(start_dim=1)\n",
    "            mse_loss = (pred_mean - flat_target).square()\n",
    "            i = t * n_steps + 1\n",
    "            alpha = self.bayesian_flow.get_alpha(i, n_steps)\n",
    "            loss = alpha * mse_loss / 2\n",
    "            \n",
    "        return n_steps * loss\n",
    "    \n",
    "    def reconstruction_loss(self, data: Tensor, output_params: Tensor, input_params: Tensor) -> Tensor:\n",
    "        output_params = sandwich(output_params)\n",
    "        flat_data = data.flatten(start_dim=1)\n",
    "        \n",
    "        # 重构损失只发生在最后时刻，于是 t=1.\n",
    "        t = torch.ones_like(data).flatten(start_dim=1).float()\n",
    "        output_dist = self.distribution_factory.get_dist(output_params, input_params, t)\n",
    "        \n",
    "        if hasattr(output_dist, \"probs\"):  # output distribution is discretized normal\n",
    "            reconstruction_loss = -output_dist.log_prob(flat_data)\n",
    "        else:  # output distribution is normal, but we use discretized normal to make results comparable (see Sec. 7.2)\n",
    "            if self.bayesian_flow.min_variance == 1e-3:  # used for 16 bin CIFAR10\n",
    "                noise_dev = 0.7 * math.sqrt(self.bayesian_flow.min_variance)\n",
    "                num_bins = 16\n",
    "            else:\n",
    "                noise_dev = math.sqrt(self.bayesian_flow.min_variance)\n",
    "                num_bins = 256\n",
    "                \n",
    "            mean = output_dist.mean.flatten(start_dim=1)\n",
    "            final_dist = D.Normal(mean, noise_dev)\n",
    "            # 离散化的正态分布\n",
    "            final_dist = DiscretizedCtsDistribution(final_dist, num_bins, device=t.device, batch_dims=mean.ndim - 1)\n",
    "            reconstruction_loss = -final_dist.log_prob(flat_data)\n",
    "            \n",
    "        return reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteBayesianFlowLoss(Loss):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bayesian_flow: DiscreteBayesianFlow,\n",
    "        distribution_factory: DiscreteDistributionFactory,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bayesian_flow = bayesian_flow\n",
    "        self.distribution_factory = distribution_factory\n",
    "        # 离散数据的输出分布建模为类别分布，这个变量就代表类别数量.\n",
    "        self.K = self.bayesian_flow.n_classes\n",
    "\n",
    "    def cts_time_loss(self, data: Tensor, output_params: Tensor, input_params: Tensor, t) -> Tensor:\n",
    "        flat_output = sandwich(output_params)\n",
    "        # 输出分布在各类别上分配的概率\n",
    "        pred_probs = self.distribution_factory.get_dist(flat_output).probs\n",
    "        \n",
    "        flat_target = data.flatten(start_dim=1)\n",
    "        if self.bayesian_flow.discretize:\n",
    "            flat_target = float_to_idx(flat_target, self.K)\n",
    "\n",
    "        tgt_mean = torch.nn.functional.one_hot(flat_target.long(), self.K)\n",
    "        kl = self.K * ((tgt_mean - pred_probs).square()).sum(-1)\n",
    "        t = t.flatten(start_dim=1).float()\n",
    "        loss = t * (self.bayesian_flow.max_sqrt_beta**2) * kl\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def discrete_time_loss(\n",
    "        self, data: Tensor, output_params: Tensor, input_params: Tensor, t: Tensor, n_steps: int, n_samples=10) -> Tensor:\n",
    "        flat_target = data.flatten(start_dim=1)\n",
    "        if self.bayesian_flow.discretize:\n",
    "            flat_target = float_to_idx(flat_target, self.K)\n",
    "        \n",
    "        # 根据 t = \\frac{i-1}{n} 反过来计算 i \n",
    "        i = t * n_steps + 1\n",
    "        # \\alpha_i\n",
    "        alpha = self.bayesian_flow.get_alpha(i, n_steps).flatten(start_dim=1)\n",
    "\n",
    "        # (B,D,K)\n",
    "        flat_output = sandwich(output_params)\n",
    "        # 模型预测的在各个类别上的概率.\n",
    "        receiver_mix_wts = self.distribution_factory.get_dist(flat_output).probs\n",
    "        # 这里之所以要在倒数第2个维度上加一维是因为以下 components 在每个类别上的均值向量都是 K 维 one-hot,\n",
    "        # 从而在每个类别上生成的是 K 个相互独立的正态分布. 总共有 K 类, 于是就有 K x K 个分布.\n",
    "        # 因此这里增加维度是为了让 categorical 权重 与 components 对齐.\n",
    "        receiver_mix_dist = D.Categorical(probs=receiver_mix_wts.unsqueeze(-2))\n",
    "        \n",
    "        # 增加2个维度是为了对应 batch dim: B 和 data dim: D.\n",
    "        classes = torch.arange(self.K, device=flat_target.device).long().unsqueeze(0).unsqueeze(0)\n",
    "        receiver_components = self.bayesian_flow.get_sender_dist(classes, alpha.unsqueeze(-1))\n",
    "        # 接收者分布, 它是多个混合高斯分布的联合分布, 其中每个数据维度都是混合高斯分布.\n",
    "        receiver_dist = D.MixtureSameFamily(receiver_mix_dist, receiver_components)\n",
    "        \n",
    "        sender_dist = self.bayesian_flow.get_sender_dist(flat_target, alpha)\n",
    "        # 从发送者分布中采样, 以蒙特卡洛方法近似估计其与接收者分布之间的 KL loss\n",
    "        y = sender_dist.sample(torch.Size([n_samples]))\n",
    "        \n",
    "        # (B,1)\n",
    "        loss = n_steps * (sender_dist.log_prob(y) - receiver_dist.log_prob(y)).mean(0).sum(-1).mean(1, keepdims=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def reconstruction_loss(self, data: Tensor, output_params: Tensor, input_params: Tensor) -> Tensor:\n",
    "        flat_outputs = sandwich(output_params)\n",
    "        flat_data = data.flatten(start_dim=1)\n",
    "        output_dist = self.distribution_factory.get_dist(flat_outputs)\n",
    "        \n",
    "        return -output_dist.log_prob(flat_data)\n",
    "\n",
    "    def float_to_idx(data: Tensor, n_classes: int) -> Tensor:\n",
    "        \"\"\"Convert continuous data to discrete indices.\"\"\"\n",
    "        # Assuming data is normalized between 0 and 1\n",
    "        return (data * n_classes).long().clamp(0, n_classes - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFN (not network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BFN(nn.Module):\n",
    "    def __init__(self, net: nn.Module, bayesian_flow: BayesianFlow, loss: Loss):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = net\n",
    "        self.bayesian_flow = bayesian_flow\n",
    "        self.loss = loss\n",
    "\n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def sample_t(data: Tensor, n_steps: Optional[int]) -> Tensor:\n",
    "        \"\"\"采样时间变量 t, 包括连续时间和离散时间两种情况.\"\"\"\n",
    "        \n",
    "        # 连续时间情况不需要指定总步数, 从 U(0,1) 连续型均匀分布中采样.\n",
    "        if n_steps == 0 or n_steps is None:\n",
    "            # (B,1)\n",
    "            t = torch.rand(data.size(0), device=data.device).unsqueeze(-1)\n",
    "        # 离散时间情况则先从 U{0,n-1} 离散型均匀分布采样出时间步，然后再除总步数 n 计算出对应的时间变量值: t = \\frac{i-1}{n}\n",
    "        # 注意, 这是每个区间起始时刻的值.\n",
    "        else:\n",
    "            # (B,1)\n",
    "            t = torch.randint(0, n_steps, (data.size(0),), device=data.device).unsqueeze(-1) / n_steps\n",
    "        # 扩展至和数据同样的维度, 不同的数据样本的时间变量不一致, 同一个样本内所有维度上所对应的时间变量则相同.\n",
    "        t = (torch.ones_like(data).flatten(start_dim=1) * t).reshape_as(data)\n",
    "        \n",
    "        return t\n",
    "\n",
    "    def forward(\n",
    "        self, data: Tensor, t: Optional[Tensor] = None, n_steps: Optional[int] = None\n",
    "    ) -> tuple[Tensor, dict[str, Tensor], Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Compute an MC estimate of the continuous (when n_steps=None or 0) or discrete time KL loss.\n",
    "        t is sampled randomly if None. If t is not None, expect t.shape == data.shape.\n",
    "        \n",
    "        使用蒙特卡洛方法估计发送者分布和接收者分布之间的 KL 散度损失:\n",
    "        -采样时间变量;\n",
    "        -从贝叶斯流分布中采样得到输入分布的参数(后验更新);\n",
    "        -将输入分布的参数喂给模型;\n",
    "        -模型返回输出分布;\n",
    "        -计算连续/离散时间 loss.\n",
    "        \"\"\"\n",
    "\n",
    "        t = self.sample_t(data, n_steps) if t is None else t\n",
    "        \n",
    "        # sample input parameter flow\n",
    "        # 从贝叶斯流分布中采样出输入分布的参数(代表已完成后验更新).\n",
    "        input_params = self.bayesian_flow(data, t)\n",
    "        # 在输入模型前转换为适合于模型输入的形式(如有必要的话)\n",
    "        net_inputs = self.bayesian_flow.params_to_net_inputs(input_params)\n",
    "        # compute output distribution parameters\n",
    "        # 注意, 这里模型输出的通常不是输出分布的参数, 而是某些变量(比如估计的噪声),\n",
    "        # 它们经过后处理才最终成为输出分布的参数.\n",
    "        output_params: Tensor = self.net(net_inputs, t)\n",
    "\n",
    "        # compute KL loss in float32\n",
    "        with torch.autocast(device_type=data.device.type if data.device.type != \"mps\" else \"cpu\", enabled=False):\n",
    "            if n_steps == 0 or n_steps is None:\n",
    "                loss = self.loss.cts_time_loss(data, output_params.float(), input_params, t)\n",
    "            else:\n",
    "                loss = self.loss.discrete_time_loss(data, output_params.float(), input_params, t, n_steps)\n",
    "\n",
    "        # loss shape is (batch_size, 1)\n",
    "        return loss.mean()\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def sample(self, data_shape: tuple, n_steps: int) -> Tensor:\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        # 起始时刻的先验\n",
    "        input_params = self.bayesian_flow.get_prior_input_params(data_shape, device)\n",
    "        distribution_factory = self.loss.distribution_factory\n",
    "\n",
    "        for i in range(1, n_steps):\n",
    "            # t_{i-1} = \\frac{i-1}{n}\n",
    "            t = torch.ones(*data_shape, device=device) * (i - 1) / n_steps\n",
    "            \n",
    "            # 模型接收输入分布的参数并预测，形成输出分布的参数后，再从其中采样作为预测(生成)的数据样本.\n",
    "            output_params = self.net(self.bayesian_flow.params_to_net_inputs(input_params), t)\n",
    "            output_sample = distribution_factory.get_dist(output_params, input_params, t).sample()\n",
    "            output_sample = output_sample.reshape(*data_shape)\n",
    "            \n",
    "            # 计算精度 \\alpha_i\n",
    "            alpha = self.bayesian_flow.get_alpha(i, n_steps)\n",
    "            # 采样观测样本\n",
    "            y = self.bayesian_flow.get_sender_dist(output_sample, alpha).sample()\n",
    "            # 后验更新\n",
    "            input_params = self.bayesian_flow.update_input_params(input_params, y, alpha)\n",
    "\n",
    "        # 最后时刻 t=1\n",
    "        t = torch.ones(*data_shape, device=device)\n",
    "        output_params = self.net(self.bayesian_flow.params_to_net_inputs(input_params), t)\n",
    "        # 概率分布的众数(mode)作为样本.\n",
    "        output_sample = distribution_factory.get_dist(output_params, input_params, t).mode\n",
    "        output_sample = output_sample.reshape(*data_shape)\n",
    "        \n",
    "        return output_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourierFeatures(nn.Module):\n",
    "    def __init__(self, first=5.0, last=6.0, step=1.0):\n",
    "        super().__init__()\n",
    "        self.freqs_exponent = torch.arange(first, last + 1e-8, step)\n",
    "\n",
    "    @property\n",
    "    def num_features(self):\n",
    "        return len(self.freqs_exponent) * 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) >= 2\n",
    "\n",
    "        # Compute (2pi * 2^n) for n in freqs.\n",
    "        freqs_exponent = self.freqs_exponent.to(dtype=x.dtype, device=x.device)  # (F, )\n",
    "        freqs = 2.0**freqs_exponent * 2 * torch.pi  # (F, )\n",
    "        freqs = freqs.view(-1, *([1] * (x.dim() - 1)))  # (F, 1, 1, ...)\n",
    "\n",
    "        # Compute (2pi * 2^n * x) for n in freqs.\n",
    "        features = freqs * x.unsqueeze(1)  # (B, F, X1, X2, ...)\n",
    "        features = features.flatten(1, 2)  # (B, F * C, X1, X2, ...)\n",
    "\n",
    "        # Output features are cos and sin of above. Shape (B, 2 * F * C, H, W).\n",
    "        return torch.cat([features.sin(), features.cos()], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_inner_heads(qkv, num_heads):\n",
    "    \"\"\"Computes attention with heads inside of qkv in the channel dimension.\n",
    "\n",
    "    Args:\n",
    "        qkv: Tensor of shape (B, 3*H*C, T) with Qs, Ks, and Vs, where:\n",
    "            H = number of heads,\n",
    "            C = number of channels per head.\n",
    "        num_heads: number of heads.\n",
    "\n",
    "    Returns:\n",
    "        Attention output of shape (B, H*C, T).\n",
    "    \"\"\"\n",
    "\n",
    "    bs, width, length = qkv.shape\n",
    "    ch = width // (3 * num_heads)\n",
    "\n",
    "    # Split into (q, k, v) of shape (B, H*C, T).\n",
    "    q, k, v = qkv.chunk(3, dim=1)\n",
    "\n",
    "    # 对 Q, K 各自缩放 1/d^{1/4} 相当于 Q, K 矩阵相乘后的结果缩放了 1/(\\sqrt{d})\n",
    "    # Rescale q and k. This makes them contiguous in memory.\n",
    "    scale = ch ** (-1 / 4)  # scale with 4th root = scaling output by sqrt\n",
    "    q = q * scale\n",
    "    k = k * scale\n",
    "\n",
    "    # Reshape qkv to (B*H, C, T).\n",
    "    new_shape = (bs * num_heads, ch, length)\n",
    "    q = q.view(*new_shape)\n",
    "    k = k.view(*new_shape)\n",
    "    v = v.reshape(*new_shape)\n",
    "\n",
    "    # Compute attention.\n",
    "    weight = einsum(\"bct,bcs->bts\", q, k)  # (B*H, T, T)\n",
    "    weight = softmax(weight.float(), dim=-1).to(weight.dtype)  # (B*H, T, T)\n",
    "    out = einsum(\"bts,bcs->bct\", weight, v)  # (B*H, C, T)\n",
    "    \n",
    "    return out.reshape(bs, num_heads * ch, length)  # (B, H*C, T)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Based on https://github.com/openai/guided-diffusion.\"\"\"\n",
    "\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        assert qkv.dim() >= 3, qkv.dim()\n",
    "        assert qkv.shape[1] % (3 * self.n_heads) == 0\n",
    "        \n",
    "        spatial_dims = qkv.shape[2:]\n",
    "        qkv = qkv.view(*qkv.shape[:2], -1)  # (B, 3*n_heads*C, T)\n",
    "        out = attention_inner_heads(qkv, self.n_heads)  # (B, n_heads*C, T)\n",
    "        \n",
    "        return out.view(*out.shape[:2], *spatial_dims).contiguous()\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Self-attention residual block.\"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, n_channels, norm_groups):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert n_channels % n_heads == 0\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups=norm_groups, num_channels=n_channels),\n",
    "            # 之所以将通道数扩展3倍是因为后续要输入到 Attention 模块, 为 Q, K ,V 各分配数量一致的通道数.\n",
    "            nn.Conv2d(n_channels, 3 * n_channels, kernel_size=1),  # (B, 3 * C, H, W)\n",
    "            Attention(n_heads),\n",
    "            # 输出卷积层初始化为全0，因此在参数更新前这部分输出特征相当于不起作用.\n",
    "            zero_init(nn.Conv2d(n_channels, n_channels, kernel_size=1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ch_in,\n",
    "        ch_out=None,\n",
    "        condition_dim=None,\n",
    "        dropout_prob=0.0,\n",
    "        norm_groups=32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        ch_out = ch_in if ch_out is None else ch_out\n",
    "        \n",
    "        self.ch_out = ch_out\n",
    "        self.condition_dim = condition_dim\n",
    "        \n",
    "        self.net1 = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups=norm_groups, num_channels=ch_in),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3, padding=1),\n",
    "        )\n",
    "        \n",
    "        if condition_dim is not None:\n",
    "            self.cond_proj = zero_init(nn.Linear(condition_dim, ch_out, bias=False))\n",
    "        \n",
    "        self.net2 = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups=norm_groups, num_channels=ch_out),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            zero_init(nn.Conv2d(ch_out, ch_out, kernel_size=3, padding=1)),\n",
    "        )\n",
    "        \n",
    "        if ch_in != ch_out:\n",
    "            self.skip_conv = nn.Conv2d(ch_in, ch_out, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        h = self.net1(x)\n",
    "        \n",
    "        if condition is not None:\n",
    "            assert condition.shape == (x.shape[0], self.condition_dim)\n",
    "            \n",
    "            # 这个条件映射层(全连接层)初始化为全0, 因此在参数更新前条件变量不起作用.\n",
    "            condition = self.cond_proj(condition)\n",
    "            # (B,D,1,1)\n",
    "            condition = condition[:, :, None, None]\n",
    "            h = h + condition\n",
    "        \n",
    "        h = self.net2(h)\n",
    "        \n",
    "        if x.shape[1] != self.ch_out:\n",
    "            x = self.skip_conv(x)\n",
    "        assert x.shape == h.shape\n",
    "        \n",
    "        return x + h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpDownBlock(nn.Module):\n",
    "    def __init__(self, resnet_block, attention_block=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.resnet_block = resnet_block\n",
    "        self.attention_block = attention_block\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        x = self.resnet_block(x, cond)\n",
    "        if self.attention_block is not None:\n",
    "            x = self.attention_block(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_float(idx: np.ndarray, num_bins: int):\n",
    "    \"\"\"将离散化区间索引 k 转换为对应的区间中心值 k_c.\n",
    "    注意, 此处 k 的取值范围与论文中的不同, 论文中 k 的取值范围是 1~K, 而这里:\n",
    "    k_c = \\frac{2k+1}{K} - 1, where k \\in [0, K-1].\"\"\"\n",
    "    \n",
    "    flt_zero_one = (idx + 0.5) / num_bins\n",
    "    return (2.0 * flt_zero_one) - 1.0\n",
    "\n",
    "\n",
    "def float_to_idx(flt: np.ndarray, num_bins: int):\n",
    "    \"\"\"根据离散化值 k_c 计算出对应的区间索引 k, 是 float_to_idx() 的逆向操作.\"\"\"\n",
    "    \n",
    "    flt_zero_one = (flt / 2.0) + 0.5\n",
    "    return torch.clamp(torch.floor(flt_zero_one * num_bins), min=0, max=num_bins - 1).long()\n",
    "\n",
    "\n",
    "def quantize(flt, num_bins: int):\n",
    "    \"\"\"将浮点值量化以对应的离散化区间中点 k_c 表示, 因此看作是一个量化的过程.\"\"\"\n",
    "    return idx_to_float(float_to_idx(flt, num_bins), num_bins)\n",
    "\n",
    "\n",
    "def rgb_image_transform(x, num_bins=256):\n",
    "    \"\"\"将 RGB 图像进行离散化, 其中 x \\in [0,1]\"\"\"\n",
    "    return quantize((x * 2) - 1, num_bins).permute(1, 2, 0).contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_from_cfg(module, cfg, **parameters):\n",
    "    return getattr(module, cfg.class_name)(**cfg.parameters, **parameters) if cfg is not None else None\n",
    "\n",
    "\n",
    "def make_bfn(cfg: DictConfig):\n",
    "    data_adapters = {\n",
    "        \"input_adapter\": make_from_cfg(adapters, cfg.input_adapter),\n",
    "        \"output_adapter\": make_from_cfg(adapters, cfg.output_adapter),\n",
    "    }\n",
    "    \n",
    "    net = make_from_cfg(networks, cfg.net, data_adapters=data_adapters)\n",
    "    bayesian_flow = make_from_cfg(model, cfg.bayesian_flow)\n",
    "    distribution_factory = make_from_cfg(probability, cfg.distribution_factory)\n",
    "    loss = make_from_cfg(model, cfg.loss, bayesian_flow=bayesian_flow, distribution_factory=distribution_factory)\n",
    "    bfn = model.BFN(net=net, bayesian_flow=bayesian_flow, loss=loss)\n",
    "    \n",
    "    return bfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(cfg) -> Tuple[nn.Module, dict, optim.Optimizer]:\n",
    "    \"\"\"Create the model, dataloader and optimizer\"\"\"\n",
    "    \n",
    "    dataloaders = make_dataloaders(cfg)\n",
    "    \n",
    "    model = make_bfn(cfg.model)    \n",
    "    if \"weight_decay\" in cfg.optimizer.keys() and hasattr(model.net, \"get_optim_groups\"):\n",
    "        # 区分了 decay 与不 decay 的参数.\n",
    "        params = model.net.get_optim_groups(cfg.optimizer.weight_decay)\n",
    "    else:\n",
    "        params = model.net.parameters()\n",
    "    \n",
    "    # Instantiate the optimizer using the hyper-parameters in the config\n",
    "    optimizer = optim.AdamW(params=params, **cfg.optimizer)\n",
    "    \n",
    "    return model, dataloaders, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import math\n",
    "\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import neptune\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from rich.logging import RichHandler\n",
    "from rich.progress import Progress\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import BFN\n",
    "from utils_train import (\n",
    "    seed_everything, log_cfg,\n",
    "    checkpoint_training_state,\n",
    "    init_checkpointing,\n",
    "    log,\n",
    "    update_ema,\n",
    "    ddict,\n",
    "    make_infinite,\n",
    "    make_progress_bar, make_config, make_dataloaders, make_bfn,\n",
    ")\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(message)s\",\n",
    "    datefmt=\"[%X]\",\n",
    "    handlers=[RichHandler(rich_tracebacks=True, show_time=False)],\n",
    ")\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "def ddict():\n",
    "    \"\"\"Infinite default dict to fake neptune run on non-main processes\"\"\"\n",
    "    return defaultdict(ddict)\n",
    "\n",
    "\n",
    "def main(cfg):\n",
    "    acc = Accelerator(gradient_accumulation_steps=cfg.training.accumulate)\n",
    "\n",
    "    cfg.training.seed = seed_everything(cfg.training.seed)\n",
    "    logger.info(f\"Seeded everything with seed {cfg.training.seed}\", main_process_only=True)\n",
    "\n",
    "    with acc.main_process_first():\n",
    "        model, dataloaders, optimizer = setup(cfg)\n",
    "        \n",
    "    ema = copy.deepcopy(model) if acc.is_main_process and cfg.training.ema_decay > 0 else None  # EMA on main proc only\n",
    "    model, optimizer, dataloaders[\"train\"] = acc.prepare(model, optimizer, dataloaders[\"train\"])\n",
    "    \n",
    "    # 这个 ddict() 对象是一个无限嵌套的 defaultdict，将其视作假的 neptune run 对象，\n",
    "    # 用于主进程之外的其它进程，类似一种 placeholder 的角色，而主进程会重新对 run 变量进行赋值，使其成为真正的neptune run 对象。\n",
    "    run = ddict()\n",
    "    \n",
    "    if acc.is_main_process:\n",
    "        ema.to(acc.device)\n",
    "        try:\n",
    "            if cfg.meta.neptune:\n",
    "                import neptune\n",
    "                \n",
    "                run = neptune.init_run(project=cfg.meta.neptune, mode=\"debug\" if cfg.meta.debug else None)\n",
    "                run[\"accelerate\"] = dict(amp=acc.mixed_precision, nproc=acc.num_processes)\n",
    "                log_cfg(cfg, run)\n",
    "        except ImportError:\n",
    "            logger.info(\"Did not find neptune installed. Logging will be disabled.\")\n",
    "\n",
    "    train(cfg.training, acc, model, ema, dataloaders, optimizer, run)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg_file = OmegaConf.from_cli()['config_file']\n",
    "    main(make_config(cfg_file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
